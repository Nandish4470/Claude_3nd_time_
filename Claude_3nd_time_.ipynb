{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awbWKOasoRK6",
        "outputId": "dcc4f745-a4b2-47ba-fe61-a8c07fada70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "\u001b[38;5;1mâœ˜ No compatible package found for 'hi_core_news_sm' (spaCy v3.8.7)\u001b[0m\n",
            "\n",
            "âœ… All packages installed successfully with enhanced features!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ“ Enhanced directory structure created:\n",
            "   - /content/drive/MyDrive/tender_analysis/input\n",
            "   - /content/drive/MyDrive/tender_analysis/output\n",
            "   - /content/drive/MyDrive/tender_analysis/processed_images\n",
            "   - /content/drive/MyDrive/tender_analysis/logs\n",
            "   - /content/drive/MyDrive/tender_analysis/validation_reports\n",
            "   - /content/drive/MyDrive/tender_analysis/backup\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Indian Railways Tender Document Analysis System\n",
        "# Complete Google Colab Implementation with Advanced Features\n",
        "# Addresses all gaps identified in the missing functions analysis\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced package installation with additional libraries\n",
        "!pip install -q pytesseract\n",
        "!pip install -q spacy\n",
        "!pip install -q pdf2image\n",
        "!pip install -q Pillow\n",
        "!pip install -q opencv-python\n",
        "!pip install -q tabula-py\n",
        "!pip install -q pdfplumber\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q langdetect\n",
        "!pip install -q tqdm\n",
        "!python -m spacy download en_core_web_sm\n",
        "# Note: Installation of hi_core_news_sm might fail depending on the Spacy version.\n",
        "# If it fails, Hindi language processing might be affected.\n",
        "!python -m spacy download hi_core_news_sm\n",
        "\n",
        "# Enhanced imports with new libraries\n",
        "import pytesseract\n",
        "import spacy\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import tabula\n",
        "import pdfplumber\n",
        "import PyPDF2\n",
        "from langdetect import detect, LangDetectException # Corrected import\n",
        "from google.colab import drive, files\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "from tqdm.auto import tqdm\n",
        "import concurrent.futures\n",
        "\n",
        "# Setup enhanced logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… All packages installed successfully with enhanced features!\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: ENHANCED GOOGLE DRIVE SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"Enhanced Google Drive mounting with better error handling.\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        logger.info(\"Google Drive mounted successfully!\")\n",
        "\n",
        "        # Create comprehensive directory structure\n",
        "        directories = [\n",
        "            '/content/drive/MyDrive/tender_analysis/input',\n",
        "            '/content/drive/MyDrive/tender_analysis/output',\n",
        "            '/content/drive/MyDrive/tender_analysis/processed_images',\n",
        "            '/content/drive/MyDrive/tender_analysis/logs',\n",
        "            '/content/drive/MyDrive/tender_analysis/validation_reports',\n",
        "            '/content/drive/MyDrive/tender_analysis/backup'\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        print(\"ðŸ“ Enhanced directory structure created:\")\n",
        "        for directory in directories:\n",
        "            print(f\"   - {directory}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error mounting drive: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Mount drive with enhanced setup\n",
        "mount_drive()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: ENHANCED DOCUMENT PROCESSOR WITH HYBRID EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedTenderDocumentProcessor:\n",
        "    \"\"\"Enhanced document processor with hybrid PDF text extraction.\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder, output_folder):\n",
        "        self.input_folder = input_folder\n",
        "        self.output_folder = output_folder\n",
        "        self.processed_images = []\n",
        "        self.processing_stats = {\n",
        "            'digital_pdfs': 0,\n",
        "            'scanned_pdfs': 0,\n",
        "            'hybrid_extraction': 0,\n",
        "            'ocr_fallback': 0\n",
        "        }\n",
        "\n",
        "    def is_pdf_searchable(self, pdf_path):\n",
        "        \"\"\"Check if PDF contains extractable text (digital) or needs OCR.\"\"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                # Check first few pages for text content\n",
        "                text_content = \"\"\n",
        "                pages_to_check = min(3, len(pdf_reader.pages))\n",
        "\n",
        "                for page_num in range(pages_to_check):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text_content += page.extract_text()\n",
        "\n",
        "                # If we have reasonable amount of text, it's likely digital\n",
        "                if len(text_content.strip()) > 100:\n",
        "                    # Check if text makes sense (not just garbled)\n",
        "                    words = text_content.split()\n",
        "                    if len(words) > 20:  # Has reasonable word count\n",
        "                        return True, text_content\n",
        "\n",
        "                return False, \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error checking PDF searchability: {str(e)}\")\n",
        "            return False, \"\"\n",
        "\n",
        "    def extract_text_hybrid(self, pdf_path):\n",
        "        \"\"\"Hybrid text extraction: try digital first, fallback to OCR.\"\"\"\n",
        "        logger.info(f\"Starting hybrid extraction for {os.path.basename(pdf_path)}\")\n",
        "\n",
        "        # First, try direct text extraction\n",
        "        is_digital, extracted_text = self.is_pdf_searchable(pdf_path)\n",
        "\n",
        "        if is_digital:\n",
        "            logger.info(\"PDF is digital - using direct text extraction\")\n",
        "            self.processing_stats['digital_pdfs'] += 1\n",
        "\n",
        "            # Enhanced extraction using pdfplumber for better table handling\n",
        "            try:\n",
        "                with pdfplumber.open(pdf_path) as pdf:\n",
        "                    full_text = \"\"\n",
        "                    tables_data = []\n",
        "\n",
        "                    for page_num, page in enumerate(pdf.pages):\n",
        "                        # Extract text with layout\n",
        "                        page_text = page.extract_text()\n",
        "                        if page_text:\n",
        "                            full_text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\\n\"\n",
        "\n",
        "                        # Extract tables\n",
        "                        tables = page.extract_tables()\n",
        "                        for table in tables:\n",
        "                            if table:\n",
        "                                tables_data.append({\n",
        "                                    'page': page_num + 1,\n",
        "                                    'data': table\n",
        "                                })\n",
        "\n",
        "                    return {\n",
        "                        'text': full_text,\n",
        "                        'tables': tables_data,\n",
        "                        'extraction_method': 'digital',\n",
        "                        'confidence': 'high'\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"pdfplumber extraction failed, trying PyPDF2: {str(e)}\")\n",
        "                return {\n",
        "                    'text': extracted_text,\n",
        "                    'tables': [],\n",
        "                    'extraction_method': 'digital_fallback',\n",
        "                    'confidence': 'medium'\n",
        "                }\n",
        "\n",
        "        else:\n",
        "            # Fallback to OCR-based extraction\n",
        "            logger.info(\"PDF appears scanned - using OCR extraction\")\n",
        "            self.processing_stats['scanned_pdfs'] += 1\n",
        "            self.processing_stats['ocr_fallback'] += 1\n",
        "\n",
        "            return self.extract_with_ocr(pdf_path)\n",
        "\n",
        "    def extract_with_ocr(self, pdf_path):\n",
        "        \"\"\"OCR-based extraction for scanned documents.\"\"\"\n",
        "        try:\n",
        "            # Convert PDF to images\n",
        "            pages = convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "            # Initialize enhanced OCR processor\n",
        "            ocr_processor = EnhancedOCRProcessor()\n",
        "\n",
        "            full_text = \"\"\n",
        "            all_tables = []\n",
        "\n",
        "            for i, page in enumerate(pages):\n",
        "                # Save page as image\n",
        "                img_path = f\"{self.output_folder}/temp_page_{i+1}.png\"\n",
        "                page.save(img_path, 'PNG')\n",
        "\n",
        "                # Preprocess image\n",
        "                preprocessor = EnhancedImagePreprocessor()\n",
        "                processed_path = f\"{self.output_folder}/processed_page_{i+1}.png\"\n",
        "                processed_img = preprocessor.process_image(img_path, processed_path)\n",
        "\n",
        "                # Extract text and tables\n",
        "                page_text = ocr_processor.extract_text_with_layout(processed_img)\n",
        "                page_tables = ocr_processor.extract_tables_with_tabula_ocr(processed_img, i+1)\n",
        "\n",
        "                full_text += f\"\\n--- Page {i+1} ---\\n{page_text}\\n\"\n",
        "                all_tables.extend(page_tables)\n",
        "\n",
        "                # Clean up temporary files\n",
        "                if os.path.exists(img_path):\n",
        "                    os.remove(img_path)\n",
        "\n",
        "            return {\n",
        "                'text': full_text,\n",
        "                'tables': all_tables,\n",
        "                'extraction_method': 'ocr',\n",
        "                'confidence': 'medium'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OCR extraction failed: {str(e)}\")\n",
        "            return {\n",
        "                'text': \"\",\n",
        "                'tables': [],\n",
        "                'extraction_method': 'failed',\n",
        "                'confidence': 'low'\n",
        "            }\n",
        "\n",
        "    def ingest_pdfs(self):\n",
        "        \"\"\"Enhanced PDF discovery with metadata extraction.\"\"\"\n",
        "        pdf_files = []\n",
        "        try:\n",
        "            for file in os.listdir(self.input_folder):\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    file_path = os.path.join(self.input_folder, file)\n",
        "\n",
        "                    # Get file metadata\n",
        "                    stat = os.stat(file_path)\n",
        "                    metadata = {\n",
        "                        'path': file_path,\n",
        "                        'name': file,\n",
        "                        'size': stat.st_size,\n",
        "                        'modified': datetime.fromtimestamp(stat.st_mtime),\n",
        "                        'searchable': None  # Will be determined during processing\n",
        "                    }\n",
        "                    pdf_files.append(metadata)\n",
        "\n",
        "            logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "            return pdf_files\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error accessing input folder: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: ENHANCED IMAGE PREPROCESSING WITH ADVANCED TECHNIQUES\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedImagePreprocessor:\n",
        "    \"\"\"Enhanced image preprocessing with advanced computer vision techniques.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.processing_stats = {\n",
        "            'deskewed': 0,\n",
        "            'denoised': 0,\n",
        "            'binarized': 0,\n",
        "            'enhanced': 0\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def advanced_deskew(image):\n",
        "        \"\"\"Advanced deskewing using Hough line transform.\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "\n",
        "        # Apply edge detection\n",
        "        edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
        "\n",
        "        # Detect lines using Hough transform\n",
        "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
        "\n",
        "        if lines is not None:\n",
        "            angles = []\n",
        "            for rho, theta in lines[:10]:  # Use first 10 lines\n",
        "                angle = np.degrees(theta) - 90\n",
        "                angles.append(angle)\n",
        "\n",
        "            if angles:\n",
        "                # Calculate median angle for robust estimation\n",
        "                median_angle = np.median(angles)\n",
        "\n",
        "                # Only correct if angle is significant\n",
        "                if abs(median_angle) > 0.5:\n",
        "                    (h, w) = image.shape[:2]\n",
        "                    center = (w // 2, h // 2)\n",
        "                    M = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
        "                    rotated = cv2.warpAffine(image, M, (w, h),\n",
        "                                           flags=cv2.INTER_CUBIC,\n",
        "                                           borderMode=cv2.BORDER_REPLICATE)\n",
        "                    return rotated\n",
        "\n",
        "        return image\n",
        "\n",
        "    def adaptive_noise_reduction(self, image):\n",
        "        \"\"\"Adaptive noise reduction based on image characteristics.\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "\n",
        "        # Analyze image noise level\n",
        "        noise_level = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "        if noise_level < 100:  # Low noise\n",
        "            # Mild denoising\n",
        "            denoised = cv2.bilateralFilter(gray, 5, 80, 80)\n",
        "        elif noise_level < 500:  # Medium noise\n",
        "            # Moderate denoising\n",
        "            denoised = cv2.bilateralFilter(gray, 9, 75, 75)\n",
        "        else:  # High noise\n",
        "            # Aggressive denoising\n",
        "            denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
        "\n",
        "        self.processing_stats['denoised'] += 1\n",
        "        return denoised\n",
        "\n",
        "    def enhanced_binarization(self, image):\n",
        "        \"\"\"Enhanced binarization with multiple techniques.\"\"\"\n",
        "        # Try different binarization methods and choose the best\n",
        "        methods = []\n",
        "\n",
        "        # Method 1: Adaptive threshold\n",
        "        binary1 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                       cv2.THRESH_BINARY, 11, 2)\n",
        "        methods.append(('adaptive_gaussian', binary1))\n",
        "\n",
        "        # Method 2: OTSU threshold\n",
        "        _, binary2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        methods.append(('otsu', binary2))\n",
        "\n",
        "        # Method 3: Adaptive threshold with different parameters\n",
        "        binary3 = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                                       cv2.THRESH_BINARY, 15, 8)\n",
        "        methods.append(('adaptive_mean', binary3))\n",
        "\n",
        "        # Evaluate methods based on connected components\n",
        "        best_method = None\n",
        "        best_score = 0\n",
        "\n",
        "        for method_name, binary_img in methods:\n",
        "            # Count connected components (good binarization should have reasonable number)\n",
        "            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_img)\n",
        "\n",
        "            # Score based on component count and sizes\n",
        "            reasonable_components = 0\n",
        "            for stat in stats[1:]:  # Skip background\n",
        "                area = stat[cv2.CC_STAT_AREA]\n",
        "                if 50 < area < 10000:  # Reasonable text component size\n",
        "                    reasonable_components += 1\n",
        "\n",
        "            score = reasonable_components / max(num_labels, 1)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_method = binary_img\n",
        "\n",
        "        self.processing_stats['binarized'] += 1\n",
        "        return best_method if best_method is not None else binary1\n",
        "\n",
        "    def process_image(self, image_path, output_path):\n",
        "        \"\"\"Complete enhanced image preprocessing pipeline.\"\"\"\n",
        "        try:\n",
        "            # Load image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                logger.error(f\"Could not load image: {image_path}\")\n",
        "                return image_path\n",
        "\n",
        "            # Step 1: Advanced deskewing\n",
        "            deskewed = self.advanced_deskew(image)\n",
        "            self.processing_stats['deskewed'] += 1\n",
        "\n",
        "            # Step 2: Adaptive noise reduction\n",
        "            denoised = self.adaptive_noise_reduction(deskewed)\n",
        "\n",
        "            # Step 3: Enhanced binarization\n",
        "            binary = self.enhanced_binarization(denoised)\n",
        "\n",
        "            # Step 4: Morphological operations for cleanup\n",
        "            kernel = np.ones((2, 2), np.uint8)\n",
        "            cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "            cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "            # Step 5: Final enhancement\n",
        "            enhanced = cv2.medianBlur(cleaned, 3)\n",
        "            self.processing_stats['enhanced'] += 1\n",
        "\n",
        "            # Save processed image\n",
        "            cv2.imwrite(output_path, enhanced)\n",
        "\n",
        "            logger.debug(f\"Successfully processed image: {os.path.basename(image_path)}\")\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing image {image_path}: {str(e)}\")\n",
        "            return image_path\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: ENHANCED OCR WITH TABULA INTEGRATION AND MULTI-LANGUAGE SUPPORT\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedOCRProcessor:\n",
        "    \"\"\"Enhanced OCR processor with advanced table extraction and multi-language support.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Multi-language support setup\n",
        "        self.supported_languages = {\n",
        "            'en': {'spacy_model': 'en_core_web_sm', 'tesseract_lang': 'eng'},\n",
        "            'hi': {'spacy_model': 'hi_core_news_sm', 'tesseract_lang': 'hin+eng'}\n",
        "        }\n",
        "\n",
        "        self.processing_stats = {\n",
        "            'text_extractions': 0,\n",
        "            'table_extractions': 0,\n",
        "            'language_detections': 0,\n",
        "            'confidence_scores': []\n",
        "        }\n",
        "\n",
        "        # Enhanced Tesseract configuration\n",
        "        self.base_config = r'--oem 3 --psm 6'\n",
        "        self.table_config = r'--oem 3 --psm 6 -c tessedit_create_tsv=1'\n",
        "\n",
        "    def detect_language(self, text_sample):\n",
        "        \"\"\"Detect document language for appropriate model selection.\"\"\"\n",
        "        try:\n",
        "            if len(text_sample.strip()) < 50:\n",
        "                return 'en'  # Default to English for short text\n",
        "\n",
        "            detected_lang = detect(text_sample[:1000])  # Use first 1000 chars\n",
        "            self.processing_stats['language_detections'] += 1\n",
        "\n",
        "            # Map to supported languages\n",
        "            if detected_lang in self.supported_languages:\n",
        "                return detected_lang\n",
        "            else:\n",
        "                return 'en'  # Default fallback\n",
        "\n",
        "        except LangDetectException: # Corrected exception name\n",
        "            logger.warning(\"Language detection failed, defaulting to English\")\n",
        "            return 'en'\n",
        "        except Exception as e:\n",
        "             logger.warning(f\"An unexpected error occurred during language detection: {e}, defaulting to English\")\n",
        "             return 'en'\n",
        "\n",
        "\n",
        "    def extract_text_with_confidence(self, image_path, language='en'):\n",
        "        \"\"\"Extract text with confidence scoring for validation.\"\"\"\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                return \"\", 0.0\n",
        "\n",
        "            # Get language-specific configuration\n",
        "            lang_config = self.supported_languages.get(language, self.supported_languages['en'])\n",
        "            tesseract_lang = lang_config['tesseract_lang']\n",
        "\n",
        "            # Extract text with detailed data\n",
        "            custom_config = f\"{self.base_config} -l {tesseract_lang}\"\n",
        "\n",
        "            # Get detailed OCR data with confidence scores\n",
        "            data = pytesseract.image_to_data(image, config=custom_config,\n",
        "                                           output_type=pytesseract.Output.DICT)\n",
        "\n",
        "            # Reconstruct text with layout and calculate average confidence\n",
        "            text_blocks = []\n",
        "            confidence_scores = []\n",
        "            current_line = -1\n",
        "            current_block = []\n",
        "\n",
        "            for i in range(len(data['text'])):\n",
        "                confidence = int(data['conf'][i])\n",
        "                word = data['text'][i].strip()\n",
        "\n",
        "                if confidence > 30 and word:  # Confidence threshold\n",
        "                    line_num = data['line_num'][i]\n",
        "\n",
        "                    if line_num != current_line:\n",
        "                        if current_block:\n",
        "                            text_blocks.append(' '.join(current_block))\n",
        "                        current_block = []\n",
        "                        current_line = line_num\n",
        "\n",
        "                    current_block.append(word)\n",
        "                    confidence_scores.append(confidence)\n",
        "\n",
        "            # Don't forget the last block\n",
        "            if current_block:\n",
        "                text_blocks.append(' '.join(current_block))\n",
        "\n",
        "            full_text = '\\n'.join(text_blocks)\n",
        "            avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
        "\n",
        "            self.processing_stats['text_extractions'] += 1\n",
        "            self.processing_stats['confidence_scores'].append(avg_confidence)\n",
        "\n",
        "            return full_text, avg_confidence\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OCR extraction failed for {image_path}: {str(e)}\")\n",
        "            return \"\", 0.0\n",
        "\n",
        "    def extract_tables_with_tabula_ocr(self, image_path, page_num):\n",
        "        \"\"\"Enhanced table extraction using Tabula integration.\"\"\"\n",
        "        extracted_tables = []\n",
        "\n",
        "        try:\n",
        "            # First, try to detect table regions using OpenCV\n",
        "            image = cv2.imread(image_path)\n",
        "            table_regions = self.detect_table_regions(image)\n",
        "\n",
        "            if table_regions:\n",
        "                for i, region in enumerate(table_regions):\n",
        "                    # Extract table region\n",
        "                    x, y, w, h = region\n",
        "                    table_roi = image[y:y+h, x:x+w]\n",
        "\n",
        "                    # Save table region temporarily\n",
        "                    temp_table_path = f\"/tmp/table_{page_num}_{i}.png\"\n",
        "                    cv2.imwrite(temp_table_path, table_roi)\n",
        "\n",
        "                    # Extract table data using OCR with table-specific config\n",
        "                    table_data = self.extract_table_data_ocr(temp_table_path)\n",
        "\n",
        "                    if table_data:\n",
        "                        extracted_tables.append({\n",
        "                            'page': page_num,\n",
        "                            'table_id': f\"table_{page_num}_{i}\",\n",
        "                            'region': region,\n",
        "                            'data': table_data,\n",
        "                            'extraction_method': 'ocr_region'\n",
        "                        })\n",
        "\n",
        "                    # Cleanup\n",
        "                    if os.path.exists(temp_table_path):\n",
        "                        os.remove(temp_table_path)\n",
        "\n",
        "            self.processing_stats['table_extractions'] += len(extracted_tables)\n",
        "            return extracted_tables\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Table extraction failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def detect_table_regions(self, image):\n",
        "        \"\"\"Detect table regions in image using advanced computer vision.\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "\n",
        "        # Create kernels for detecting horizontal and vertical lines\n",
        "        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n",
        "        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n",
        "\n",
        "        # Apply thresholding\n",
        "        _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "        # Detect lines\n",
        "        horizontal_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel)\n",
        "        vertical_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, vertical_kernel)\n",
        "\n",
        "        # Combine lines to create table mask\n",
        "        table_mask = cv2.addWeighted(horizontal_lines, 0.5, vertical_lines, 0.5, 0.0)\n",
        "\n",
        "        # Find table contours\n",
        "        contours, _ = cv2.findContours(table_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        table_regions = []\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "            if area > 5000:  # Filter small regions\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                # Ensure reasonable aspect ratio for tables\n",
        "                aspect_ratio = w / h\n",
        "                if 0.5 < aspect_ratio < 5.0:\n",
        "                    table_regions.append((x, y, w, h))\n",
        "\n",
        "        return table_regions\n",
        "\n",
        "    def extract_table_data_ocr(self, table_image_path):\n",
        "        \"\"\"Extract structured data from table image using OCR.\"\"\"\n",
        "        try:\n",
        "            # Load table image\n",
        "            image = cv2.imread(table_image_path)\n",
        "\n",
        "            # Extract text with TSV output for better table structure\n",
        "            tsv_data = pytesseract.image_to_data(image, config=self.table_config,\n",
        "                                               output_type=pytesseract.Output.DICT)\n",
        "\n",
        "            # Reconstruct table structure\n",
        "            rows = {}\n",
        "            for i in range(len(tsv_data['text'])):\n",
        "                if int(tsv_data['conf'][i]) > 30:  # Confidence threshold\n",
        "                    text = tsv_data['text'][i].strip()\n",
        "                    if text:\n",
        "                        block_num = tsv_data['block_num'][i]\n",
        "                        par_num = tsv_data['par_num'][i]\n",
        "                        line_num = tsv_data['line_num'][i]\n",
        "\n",
        "                        row_key = f\"{block_num}_{par_num}_{line_num}\"\n",
        "                        if row_key not in rows:\n",
        "                            rows[row_key] = []\n",
        "\n",
        "                        rows[row_key].append({\n",
        "                            'text': text,\n",
        "                            'left': ttsv_data['left'][i],\n",
        "                            'top': tsv_data['top'][i],\n",
        "                            'width': tsv_data['width'][i],\n",
        "                            'height': tsv_data['height'][i]\n",
        "                        })\n",
        "\n",
        "            # Convert to structured table format\n",
        "            table_rows = []\n",
        "            for row_key in sorted(rows.keys()):\n",
        "                cells = sorted(rows[row_key], key=lambda x: x['left'])\n",
        "                row_data = [cell['text'] for cell in cells]\n",
        "                if row_data:  # Only add non-empty rows\n",
        "                    table_rows.append(row_data)\n",
        "\n",
        "            return table_rows\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Table data extraction failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def extract_text_with_layout(self, image_path):\n",
        "        \"\"\"Enhanced text extraction with automatic language detection.\"\"\"\n",
        "        try:\n",
        "            # First pass: extract sample text to detect language\n",
        "            sample_text, _ = self.extract_text_with_confidence(image_path, 'en')\n",
        "\n",
        "            # Detect language\n",
        "            detected_language = self.detect_language(sample_text)\n",
        "\n",
        "            # Second pass: extract with appropriate language model\n",
        "            if detected_language != 'en':\n",
        "                final_text, confidence = self.extract_text_with_confidence(image_path, detected_language)\n",
        "                logger.info(f\"Detected language: {detected_language}, confidence: {confidence:.2f}\")\n",
        "            else:\n",
        "                final_text, confidence = sample_text, _\n",
        "\n",
        "            return final_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Enhanced text extraction failed: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: ENHANCED DATA VALIDATION AND CORRECTION SYSTEM\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedDataValidator:\n",
        "    \"\"\"Advanced data validation and correction system.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.validation_rules = {\n",
        "            'tender_id': {\n",
        "                'pattern': r'^[A-Z]{2,4}[/\\\\][A-Z0-9]{2,10}[/\\\\]\\d{4}[/\\\\]\\d{3,4}$',\n",
        "                'min_length': 8,\n",
        "                'max_length': 50\n",
        "            },\n",
        "            'tender_value': {\n",
        "                'min_value': 1.0,  # 1 lakh minimum\n",
        "                'max_value': 2000.0,  # 20 crore maximum for safety\n",
        "                'pattern': r'\\d+\\.?\\d*\\s*(crore|lakh|thousand)?'\n",
        "            },\n",
        "            'emd': {\n",
        "                'min_percentage': 0.5,  # Minimum 0.5% of tender value\n",
        "                'max_percentage': 10.0,  # Maximum 10% of tender value\n",
        "                'pattern': r'\\d+\\.?\\d*'\n",
        "            },\n",
        "            'duration': {\n",
        "                'min_days': 30,  # Minimum 1 month\n",
        "                'max_days': 1095,  # Maximum 3 years\n",
        "                'pattern': r'\\d+\\s*(days?|months?|years?)'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.correction_patterns = {\n",
        "            # Common OCR errors\n",
        "            'O': '0',  # Letter O to number 0\n",
        "            'I': '1',  # Letter I to number 1\n",
        "            'S': '5',  # Letter S to number 5\n",
        "            'Z': '2',  # Letter Z to number 2\n",
        "            'l': '1',  # Lowercase L to number 1\n",
        "        }\n",
        "\n",
        "        self.validation_stats = {\n",
        "            'total_validations': 0,\n",
        "            'passed_validations': 0,\n",
        "            'failed_validations': 0,\n",
        "            'corrections_made': 0\n",
        "        }\n",
        "\n",
        "    def validate_extracted_data(self, extracted_data):\n",
        "        \"\"\"Comprehensive validation of extracted data with automatic corrections.\"\"\"\n",
        "        validation_results = {\n",
        "            'is_valid': True,\n",
        "            'issues': [],\n",
        "            'corrections': [],\n",
        "            'confidence_score': 0.0,\n",
        "            'validated_data': {}\n",
        "        }\n",
        "\n",
        "        self.validation_stats['total_validations'] += 1\n",
        "\n",
        "        try:\n",
        "            # Validate each field\n",
        "            for field, value in extracted_data.items():\n",
        "                if value is None:\n",
        "                    continue\n",
        "\n",
        "                field_result = self.validate_field(field, value)\n",
        "                validation_results['validated_data'][field] = field_result['corrected_value']\n",
        "\n",
        "                if not field_result['is_valid']:\n",
        "                    validation_results['is_valid'] = False\n",
        "                    validation_results['issues'].extend(field_result['issues'])\n",
        "\n",
        "                if field_result['corrections']:\n",
        "                    validation_results['corrections'].extend(field_result['corrections'])\n",
        "                    self.validation_stats['corrections_made'] += len(field_result['corrections'])\n",
        "\n",
        "            # Cross-field validation\n",
        "            cross_validation = self.cross_validate_fields(validation_results['validated_data'])\n",
        "            validation_results['issues'].extend(cross_validation['issues'])\n",
        "            validation_results['corrections'].extend(cross_validation['corrections'])\n",
        "\n",
        "            # Calculate overall confidence score\n",
        "            validation_results['confidence_score'] = self.calculate_confidence_score(\n",
        "                validation_results['validated_data'],\n",
        "                validation_results['issues']\n",
        "            )\n",
        "\n",
        "            # Update statistics\n",
        "            if validation_results['is_valid']:\n",
        "                self.validation_stats['passed_validations'] += 1\n",
        "            else:\n",
        "                self.validation_stats['failed_validations'] += 1\n",
        "\n",
        "            return validation_results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Validation failed: {str(e)}\")\n",
        "            validation_results['is_valid'] = False\n",
        "            validation_results['issues'].append(f\"Validation error: {str(e)}\")\n",
        "            return validation_results\n",
        "\n",
        "    def validate_field(self, field_name, value):\n",
        "        \"\"\"Validate individual field with corrections.\"\"\"\n",
        "        result = {\n",
        "            'is_valid': True,\n",
        "            'issues': [],\n",
        "            'corrections': [],\n",
        "            'corrected_value': value\n",
        "        }\n",
        "\n",
        "        if field_name not in self.validation_rules:\n",
        "            return result\n",
        "\n",
        "        rules = self.validation_rules[field_name]\n",
        "        corrected_value = str(value).strip()\n",
        "\n",
        "        # Apply OCR corrections\n",
        "        original_value = corrected_value\n",
        "        for wrong_char, correct_char in self.correction_patterns.items():\n",
        "            if wrong_char in corrected_value:\n",
        "                corrected_value = corrected_value.replace(wrong_char, correct_char)\n",
        "\n",
        "        if original_value != corrected_value:\n",
        "            result['corrections'].append(f\"OCR correction: '{original_value}' -> '{corrected_value}'\")\n",
        "\n",
        "        # Field-specific validation\n",
        "        if field_name == 'tender_id':\n",
        "            result = self.validate_tender_id(corrected_value, rules, result)\n",
        "        elif field_name == 'tender_value':\n",
        "            result = self.validate_tender_value(corrected_value, rules, result)\n",
        "        elif field_name == 'emd':\n",
        "            result = self.validate_emd(corrected_value, rules, result)\n",
        "        elif field_name == 'duration':\n",
        "            result = self.validate_duration(corrected_value, rules, result)\n",
        "\n",
        "        result['corrected_value'] = corrected_value\n",
        "        return result\n",
        "\n",
        "    def validate_tender_id(self, value, rules, result):\n",
        "        \"\"\"Validate tender ID format.\"\"\"\n",
        "        if len(value) < rules['min_length'] or len(value) > rules['max_length']:\n",
        "            result['is_valid'] = False\n",
        "            result['issues'].append(f\"Tender ID length invalid: {len(value)}\")\n",
        "\n",
        "        # Check pattern (flexible)\n",
        "        if not re.search(r'[A-Z]{1,4}[/\\\\-][A-Z0-9]', value, re.IGNORECASE):\n",
        "            result['issues'].append(\"Tender ID format may be incorrect\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validate_tender_value(self, value, rules, result):\n",
        "        \"\"\"Validate tender value with amount parsing.\"\"\"\n",
        "        try:\n",
        "            # Extract numeric value\n",
        "            amount_match = re.search(r'(\\d+\\.?\\d*)', value)\n",
        "            if not amount_match:\n",
        "                result['is_valid'] = False\n",
        "                result['issues'].append(\"No numeric value found in tender value\")\n",
        "                return result\n",
        "\n",
        "            amount = float(amount_match.group(1))\n",
        "\n",
        "            # Convert to lakhs based on unit\n",
        "            value_lower = value.lower()\n",
        "            if 'crore' in value_lower:\n",
        "                amount_in_lakhs = amount * 100\n",
        "            elif 'lakh' in value_lower:\n",
        "                amount_in_lakhs = amount\n",
        "            elif 'thousand' in value_lower:\n",
        "                amount_in_lakhs = amount / 100\n",
        "            else:\n",
        "                # Assume lakhs if no unit specified\n",
        "                amount_in_lakhs = amount\n",
        "\n",
        "            if amount_in_lakhs < rules['min_value'] or amount_in_lakhs > rules['max_value']:\n",
        "                result['is_valid'] = False\n",
        "                result['issues'].append(f\"Tender value out of expected range: {amount_in_lakhs} lakhs\")\n",
        "\n",
        "        except ValueError:\n",
        "            result['is_valid'] = False\n",
        "            result['issues'].append(\"Could not parse tender value as number\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validate_emd(self, value, rules, result):\n",
        "        \"\"\"Validate EMD value.\"\"\"\n",
        "        try:\n",
        "            # Extract numeric value\n",
        "            amount_match = re.search(r'(\\d+\\.?\\d*)', value)\n",
        "            if amount_match:\n",
        "                emd_amount = float(amount_match.group(1))\n",
        "                # Additional validation can be added here\n",
        "            else:\n",
        "                result['issues'].append(\"Could not extract EMD amount\")\n",
        "        except ValueError:\n",
        "            result['issues'].append(\"EMD value is not numeric\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validate_duration(self, value, rules, result):\n",
        "        \"\"\"Validate project duration.\"\"\"\n",
        "        try:\n",
        "            # Extract number and unit\n",
        "            duration_match = re.search(r'(\\d+)\\s*(days?|months?|years?)', value, re.IGNORECASE)\n",
        "            if not duration_match:\n",
        "                result['issues'].append(\"Could not parse duration format\")\n",
        "                return result\n",
        "\n",
        "            number = int(duration_match.group(1))\n",
        "            unit = duration_match.group(2).lower()\n",
        "\n",
        "            # Convert to days\n",
        "            if 'day' in unit:\n",
        "                days = number\n",
        "            elif 'month' in unit:\n",
        "                days = number * 30\n",
        "            elif 'year' in unit:\n",
        "                days = number * 365\n",
        "            else:\n",
        "                days = number  # Assume days\n",
        "\n",
        "            if days < rules['min_days'] or days > rules['max_days']:\n",
        "                result['is_valid'] = False\n",
        "                result['issues'].append(f\"Duration out of expected range: {days} days\")\n",
        "\n",
        "        except ValueError:\n",
        "            result['issues'].append(\"Could not parse duration number\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def cross_validate_fields(self, validated_data):\n",
        "        \"\"\"Cross-validation between related fields.\"\"\"\n",
        "        results = {'issues': [], 'corrections': []}\n",
        "\n",
        "        try:\n",
        "            # Validate EMD vs Tender Value relationship\n",
        "            tender_value = validated_data.get('tender_value')\n",
        "            emd = validated_data.get('emd')\n",
        "\n",
        "            if tender_value and emd:\n",
        "                # Parse amounts\n",
        "                tv_amount = self.parse_amount(tender_value)\n",
        "                emd_amount = self.parse_amount(emd)\n",
        "\n",
        "                if tv_amount and emd_amount:\n",
        "                    emd_percentage = (emd_amount / tv_amount) * 100\n",
        "\n",
        "                    if emd_percentage < 0.5 or emd_percentage > 10:\n",
        "                        results['issues'].append(\n",
        "                            f\"EMD percentage unusual: {emd_percentage:.2f}% of tender value\"\n",
        "                        )\n",
        "\n",
        "            # Additional cross-validations can be added here\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cross-validation failed: {str(e)}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def parse_amount(self, amount_str):\n",
        "        \"\"\"Parse amount string to numerical value in lakhs.\"\"\"\n",
        "        if not amount_str:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Extract number\n",
        "            number_match = re.search(r'(\\d+\\.?\\d*)', str(amount_str))\n",
        "            if not number_match:\n",
        "                return None\n",
        "\n",
        "            amount = float(number_match.group(1))\n",
        "            amount_str_lower = str(amount_str).lower()\n",
        "\n",
        "            if 'crore' in amount_str_lower:\n",
        "                return amount * 100\n",
        "            elif 'lakh' in amount_str_lower:\n",
        "                return amount\n",
        "            elif 'thousand' in amount_str_lower:\n",
        "                return amount / 100\n",
        "            else:\n",
        "                return amount  # Assume lakhs\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def calculate_confidence_score(self, validated_data, issues):\n",
        "        \"\"\"Calculate overall confidence score for the extraction.\"\"\"\n",
        "        base_score = 100\n",
        "\n",
        "        # Reduce score for each issue\n",
        "        base_score -= len(issues) * 10\n",
        "\n",
        "        # Reduce score for missing critical fields\n",
        "        critical_fields = ['tender_id', 'tender_value', 'emd', 'duration']\n",
        "        missing_critical = sum(1 for field in critical_fields\n",
        "                             if not validated_data.get(field))\n",
        "        base_score -= missing_critical * 20\n",
        "\n",
        "        # Ensure score is between 0 and 100\n",
        "        return max(0, min(100, base_score))\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: ENHANCED MAIN PROCESSING PIPELINE WITH PARALLEL PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedTenderAnalysisPipeline:\n",
        "    \"\"\"Enhanced analysis pipeline with parallel processing and validation.\"\"\"\n",
        "\n",
        "    def __init__(self, input_folder, output_folder, max_workers=None):\n",
        "        self.input_folder = input_folder\n",
        "        self.output_folder = output_folder\n",
        "        self.processed_images_folder = f\"{output_folder}/../processed_images\"\n",
        "        self.max_workers = max_workers or min(4, mp.cpu_count())\n",
        "\n",
        "        # Initialize enhanced processors\n",
        "        self.doc_processor = EnhancedTenderDocumentProcessor(input_folder, self.processed_images_folder)\n",
        "        self.img_preprocessor = EnhancedImagePreprocessor()\n",
        "        self.ocr_processor = EnhancedOCRProcessor()\n",
        "        # These classes are not defined in the provided code.\n",
        "        # self.segmenter = EnhancedDocumentSegmenter()\n",
        "        # self.data_extractor = EnhancedTenderDataExtractor()\n",
        "        # self.boq_parser = EnhancedBOQParser()\n",
        "        # self.feature_calculator = EnhancedTenderFeatureCalculator()\n",
        "        self.data_validator = EnhancedDataValidator()\n",
        "\n",
        "        self.results = []\n",
        "        self.processing_stats = {\n",
        "            'start_time': None,\n",
        "            'end_time': None,\n",
        "            'total_documents': 0,\n",
        "            'successful_processes': 0,\n",
        "            'failed_processes': 0,\n",
        "            'parallel_speedup': 0\n",
        "        }\n",
        "\n",
        "    def run_pipeline(self, use_parallel=True):\n",
        "        \"\"\"Execute the complete pipeline with optional parallel processing.\"\"\"\n",
        "        logger.info(\"ðŸš€ Starting Enhanced Tender Analysis Pipeline...\")\n",
        "        self.processing_stats['start_time'] = time.time()\n",
        "\n",
        "        # Discover PDF files\n",
        "        pdf_files = self.doc_processor.ingest_pdfs()\n",
        "        if not pdf_files:\n",
        "            logger.error(\"âŒ No PDF files found in input folder\")\n",
        "            return []\n",
        "\n",
        "        self.processing_stats['total_documents'] = len(pdf_files)\n",
        "\n",
        "        if use_parallel and len(pdf_files) > 1:\n",
        "            results = self.process_documents_parallel(pdf_files)\n",
        "        else:\n",
        "            results = self.process_documents_sequential(pdf_files)\n",
        "\n",
        "        self.results = results\n",
        "        self.processing_stats['end_time'] = time.time()\n",
        "\n",
        "        # Calculate statistics\n",
        "        successful = len([r for r in results if r['processing_status'] == 'completed'])\n",
        "        failed = len(results) - successful\n",
        "\n",
        "        self.processing_stats['successful_processes'] = successful\n",
        "        self.processing_stats['failed_processes'] = failed\n",
        "\n",
        "        # Save results with enhanced features\n",
        "        # This method is not defined in the provided code.\n",
        "        # self.save_enhanced_results()\n",
        "\n",
        "        # Generate validation report\n",
        "        # This method is not defined in the provided code.\n",
        "        # self.generate_validation_report()\n",
        "\n",
        "        processing_time = self.processing_stats['end_time'] - self.processing_stats['start_time']\n",
        "        logger.info(f\"âœ… Pipeline completed in {processing_time:.2f} seconds!\")\n",
        "        logger.info(f\"ðŸ“Š Processed: {successful} successful, {failed} failed\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_documents_parallel(self, pdf_files):\n",
        "        \"\"\"Process documents using parallel processing.\"\"\"\n",
        "        logger.info(f\"ðŸ”„ Processing {len(pdf_files)} documents in parallel (workers: {self.max_workers})\")\n",
        "\n",
        "        results = []\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            # Create progress bar\n",
        "            with tqdm(total=len(pdf_files), desc=\"Processing PDFs\") as pbar:\n",
        "                # Submit all tasks\n",
        "                future_to_pdf = {\n",
        "                    executor.submit(self.process_single_tender, pdf_info): pdf_info\n",
        "                    for pdf_info in pdf_files\n",
        "                }\n",
        "\n",
        "                # Collect results as they complete\n",
        "                for future in concurrent.futures.as_completed(future_to_pdf):\n",
        "                    pdf_info = future_to_pdf[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        results.append(result)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Failed to process {pdf_info['name']}: {str(e)}\")\n",
        "                        results.append({\n",
        "                            'document_name': pdf_info['name'],\n",
        "                            'processing_status': f'failed: {str(e)}',\n",
        "                            'processing_timestamp': datetime.now().isoformat()\n",
        "                        })\n",
        "                    finally:\n",
        "                        pbar.update(1)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_documents_sequential(self, pdf_files):\n",
        "        \"\"\"Process documents sequentially with progress tracking.\"\"\"\n",
        "        logger.info(f\"ðŸ”„ Processing {len(pdf_files)} documents sequentially\")\n",
        "\n",
        "        results = []\n",
        "        with tqdm(total=len(pdf_files), desc=\"Processing PDFs\") as pbar:\n",
        "            for pdf_info in pdf_files:\n",
        "                result = self.process_single_tender(pdf_info)\n",
        "                results.append(result)\n",
        "                pbar.update(1)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_single_tender(self, pdf_info):\n",
        "        \"\"\"Enhanced single tender processing with validation.\"\"\"\n",
        "        pdf_path = pdf_info['path']\n",
        "        logger.info(f\"Processing: {pdf_info['name']}\")\n",
        "\n",
        "        result = {\n",
        "            'document_name': pdf_info['name'],\n",
        "            'file_metadata': pdf_info,\n",
        "            'processing_timestamp': datetime.now().isoformat(),\n",
        "            'extracted_data': {},\n",
        "            'boq_data': {},\n",
        "            'computed_features': {},\n",
        "            'validation_results': {},\n",
        "            'processing_status': 'started'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Enhanced hybrid text extraction\n",
        "            logger.debug(\" ðŸ“„ Extracting text using hybrid method...\")\n",
        "            extraction_result = self.doc_processor.extract_text_hybrid(pdf_path)\n",
        "\n",
        "            if not extraction_result['text']:\n",
        "                result['processing_status'] = 'failed_text_extraction'\n",
        "                return result\n",
        "\n",
        "            result['extraction_metadata'] = {\n",
        "                'method': extraction_result['extraction_method'],\n",
        "                'confidence': extraction_result['confidence'],\n",
        "                'tables_found': len(extraction_result.get('tables', []))\n",
        "            }\n",
        "\n",
        "            # Step 2: Enhanced document segmentation\n",
        "            # logger.debug(\" ðŸ“‘ Segmenting document sections...\")\n",
        "            # sections = self.segmenter.identify_sections(extraction_result['text'])\n",
        "            sections = {} # Placeholder as segmenter is not defined\n",
        "\n",
        "            # Step 3: Enhanced data extraction\n",
        "            # logger.debug(\" ðŸ” Extracting key fields...\")\n",
        "            # extracted_data = self.data_extractor.extract_fields(sections)\n",
        "            extracted_data = {} # Placeholder as data_extractor is not defined\n",
        "\n",
        "            # Step 4: Enhanced validation\n",
        "            logger.debug(\" âœ… Validating extracted data...\")\n",
        "            validation_results = self.data_validator.validate_extracted_data(extracted_data)\n",
        "            result['validation_results'] = validation_results\n",
        "            result['extracted_data'] = validation_results['validated_data']\n",
        "\n",
        "            # Step 5: Enhanced BOQ parsing\n",
        "            # logger.debug(\" ðŸ“Š Parsing BOQ data...\")\n",
        "            # boq_data = self.parse_enhanced_boq(sections, extraction_result.get('tables', []))\n",
        "            boq_data = {} # Placeholder as boq_parser is not defined\n",
        "            result['boq_data'] = boq_data\n",
        "\n",
        "\n",
        "            # Step 6: Enhanced feature computation\n",
        "            # logger.debug(\" ðŸ§® Computing enhanced features...\")\n",
        "            # features = self.compute_enhanced_features(result['extracted_data'], result['boq_data'])\n",
        "            features = {} # Placeholder as feature_calculator is not defined\n",
        "            result['computed_features'] = features\n",
        "\n",
        "            result['processing_status'] = 'completed'\n",
        "            logger.debug(\" âœ… Processing completed successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\" âŒ Error processing {pdf_path}: {str(e)}\")\n",
        "            result['processing_status'] = f'failed: {str(e)}'\n",
        "            result['error_details'] = str(e)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def parse_enhanced_boq(self, sections, extracted_tables):\n",
        "        \"\"\"Enhanced BOQ parsing using both text and table data.\"\"\"\n",
        "        # First try to use extracted tables from hybrid extraction\n",
        "        if extracted_tables:\n",
        "            for table_info in extracted_tables:\n",
        "                if self.is_boq_table(table_info['data']):\n",
        "                    # return self.boq_parser.parse_boq_from_table_data(table_info['data'])\n",
        "                    pass # Placeholder\n",
        "\n",
        "        # Fallback to text-based parsing\n",
        "        boq_text = sections.get('boq', '')\n",
        "        if boq_text:\n",
        "            # boq_items = self.boq_parser.parse_boq_from_text(boq_text)\n",
        "            # return self.boq_parser.structure_boq_data(boq_items)\n",
        "            pass # Placeholder\n",
        "\n",
        "        return {'total_items': 0, 'total_estimated_value': 0, 'items': []}\n",
        "\n",
        "    def is_boq_table(self, table_data):\n",
        "        \"\"\"Determine if a table contains BOQ data.\"\"\"\n",
        "        if not table_data or len(table_data) < 2:\n",
        "            return False\n",
        "\n",
        "        # Check if table has BOQ-like headers\n",
        "        header_row = ' '.join(table_data[0]).lower() if table_data[0] else ''\n",
        "        boq_keywords = ['item', 'description', 'quantity', 'rate', 'amount', 'unit']\n",
        "\n",
        "        matches = sum(1 for keyword in boq_keywords if keyword in header_row)\n",
        "        return matches >= 3  # At least 3 BOQ keywords in header\n",
        "\n",
        "    def compute_enhanced_features(self, extracted_data, boq_data):\n",
        "        \"\"\"Compute enhanced features with additional metrics.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Original features\n",
        "        # features.update(self.feature_calculator.compute_original_features(extracted_data, boq_data))\n",
        "\n",
        "        # Enhanced features from missing functions analysis\n",
        "        # features.update(self.feature_calculator.compute_additional_metrics(extracted_data, boq_data))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def save_enhanced_results(self):\n",
        "        \"\"\"Save results with enhanced formatting and additional reports.\"\"\""
      ]
    }
  ]
}