{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe60Vjil6OWlijcY9GMBPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandish4470/Claude_3nd_time_/blob/main/Analyzer_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust parsing helpers to replace brittle table/line parsing in Analyzer_2_1.ipynb\n",
        "# Save this file in the repository (e.g., at the repo root or in a `utils/` folder)\n",
        "# and import it from the notebook:\n",
        "#   from parser_fixes import safe_normalize_and_parse, build_item_breakup, to_number\n",
        "#\n",
        "# This module:\n",
        "# - improves numeric parsing (currency, grouping, parentheses)\n",
        "# - normalizes DataFrame columns safely (no DataFrame.str)\n",
        "# - provides parse_item_line with strict + heuristic fallbacks\n",
        "# - builds the canonical item-breakup DataFrame with columns:\n",
        "#   [\"S No.\", \"Item No\", \"Description of Item\", \"Unit\", \"Qty\", \"Rate\", \"Amount\"]\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Improved numeric conversion with currency, grouping and parentheses support\n",
        "def to_number(value, default=np.nan):\n",
        "    if value is None:\n",
        "        return default\n",
        "    if isinstance(value, (int, float, np.floating, np.integer)):\n",
        "        try:\n",
        "            return float(value)\n",
        "        except Exception:\n",
        "            return default\n",
        "    s = str(value).strip()\n",
        "    if s == \"\" or s.lower() in (\"na\", \"n/a\", \"-\"):\n",
        "        return default\n",
        "    # Remove common currency symbols and non-breaking spaces\n",
        "    s = s.replace('\\xa0', '').replace('$', '').replace('€', '').replace('£', '').replace('₹', '')\n",
        "    # Handle parentheses as negative\n",
        "    neg = False\n",
        "    if s.startswith('(') and s.endswith(')'):\n",
        "        neg = True\n",
        "        s = s[1:-1].strip()\n",
        "    # Remove commas used as thousands separators (but keep decimal points)\n",
        "    s = s.replace(',', '')\n",
        "    # Final sanitization: keep digits, dot, minus, plus, and exponent\n",
        "    s = re.sub(r'[^0-9eE\\.\\-+]', '', s)\n",
        "    if s in (\"\", \".\", \"-\", \"+\"):\n",
        "        return default\n",
        "    try:\n",
        "        num = float(s)\n",
        "        return -num if neg else num\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# Normalize a parsed DataFrame safely (do not call DataFrame.str)\n",
        "def normalize_table_df(df):\n",
        "    df = df.copy()\n",
        "    # Ensure column names are strings\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n",
        "            # Use map instead of applymap on entire DataFrame\n",
        "            df[col] = df[col].astype(str).map(lambda x: x.strip())\n",
        "    return df\n",
        "\n",
        "# Primary robust parser for a single free-text item line\n",
        "def parse_item_line(text_line):\n",
        "    \"\"\"\n",
        "    Parse a single line which may contain:\n",
        "    S No., Item No, Description, Unit, Qty, Rate, Amount\n",
        "    Returns a dict with keys:\n",
        "      \"S No.\", \"Item No\", \"Description of Item\", \"Unit\", \"Qty\", \"Rate\", \"Amount\"\n",
        "    Fields not found are None.\n",
        "    \"\"\"\n",
        "    if text_line is None:\n",
        "        return None\n",
        "    line = str(text_line).strip()\n",
        "    if line == \"\":\n",
        "        return None\n",
        "\n",
        "    # 1) Strict regex for fully-structured lines\n",
        "    strict_re = re.compile(\n",
        "        r'^\\s*(?P<sno>\\d{1,6})\\s+'\n",
        "        r'(?P<itemno>[\\w\\-\\./]{1,30})\\s+'\n",
        "        r'(?P<desc>.+?)\\s+'\n",
        "        r'(?P<unit>[A-Za-z/%]{1,10})\\s+'\n",
        "        r'(?P<qty>[\\d,().\\u00A0-]+)\\s+'\n",
        "        r'(?P<rate>[\\d,().\\u00A0-]+)\\s+'\n",
        "        r'(?P<amount>[\\d,().\\u00A0-]+)\\s*$'\n",
        "    )\n",
        "    m = strict_re.match(line)\n",
        "    if m:\n",
        "        return {\n",
        "            \"S No.\": m.group('sno'),\n",
        "            \"Item No\": m.group('itemno'),\n",
        "            \"Description of Item\": m.group('desc').strip(),\n",
        "            \"Unit\": m.group('unit'),\n",
        "            \"Qty\": to_number(m.group('qty')),\n",
        "            \"Rate\": to_number(m.group('rate')),\n",
        "            \"Amount\": to_number(m.group('amount'))\n",
        "        }\n",
        "\n",
        "    # 2) Looser approach: extract trailing numeric tokens (likely Qty, Rate, Amount)\n",
        "    tokens = re.split(r'\\s{2,}|\\t', line)\n",
        "    if len(tokens) == 1:\n",
        "        tokens = line.split()\n",
        "\n",
        "    flat_tokens = []\n",
        "    for t in tokens:\n",
        "        parts = t.strip().split()\n",
        "        flat_tokens.extend([p for p in parts if p != \"\"])\n",
        "\n",
        "    def is_numeric_like(tok):\n",
        "        return bool(re.search(r'\\d', tok)) and not re.match(r'^[A-Za-z]+$', tok)\n",
        "\n",
        "    parsed = {\n",
        "        \"S No.\": None,\n",
        "        \"Item No\": None,\n",
        "        \"Description of Item\": None,\n",
        "        \"Unit\": None,\n",
        "        \"Qty\": None,\n",
        "        \"Rate\": None,\n",
        "        \"Amount\": None\n",
        "    }\n",
        "\n",
        "    if len(flat_tokens) >= 3:\n",
        "        last3 = flat_tokens[-3:]\n",
        "        if all(is_numeric_like(t) for t in last3):\n",
        "            parsed[\"Qty\"] = to_number(last3[0])\n",
        "            parsed[\"Rate\"] = to_number(last3[1])\n",
        "            parsed[\"Amount\"] = to_number(last3[2])\n",
        "            prefix = flat_tokens[:-3]\n",
        "            if len(prefix) >= 2 and re.match(r'^\\d+$', prefix[0]):\n",
        "                parsed[\"S No.\"] = prefix[0]\n",
        "                parsed[\"Item No\"] = prefix[1]\n",
        "                parsed[\"Description of Item\"] = \" \".join(prefix[2:]) if len(prefix) > 2 else \"\"\n",
        "            else:\n",
        "                if len(prefix) >= 1 and re.match(r'^[\\w\\-\\./]+$', prefix[0]):\n",
        "                    parsed[\"Item No\"] = prefix[0]\n",
        "                    parsed[\"Description of Item\"] = \" \".join(prefix[1:]) if len(prefix) > 1 else \"\"\n",
        "                else:\n",
        "                    parsed[\"Description of Item\"] = \" \".join(prefix)\n",
        "            if parsed[\"Description of Item\"]:\n",
        "                desc_tokens = parsed[\"Description of Item\"].split()\n",
        "                if len(desc_tokens) >= 1 and re.fullmatch(r'[A-Za-z/%]{1,6}', desc_tokens[-1]):\n",
        "                    parsed[\"Unit\"] = desc_tokens[-1]\n",
        "                    parsed[\"Description of Item\"] = \" \".join(desc_tokens[:-1]).strip()\n",
        "            return parsed\n",
        "\n",
        "    # 3) Pattern where only amount is explicit\n",
        "    alt_re = re.compile(\n",
        "        r'^(?:(?P<sno>\\d{1,6})\\s+)?(?:(?P<itemno>[\\w\\-\\./]{1,30})\\s+)?(?P<desc>.+?)\\s+'\n",
        "        r'(?P<amount>[\\d,().\\u00A0\\-]+)$'\n",
        "    )\n",
        "    m2 = alt_re.match(line)\n",
        "    if m2:\n",
        "        parsed[\"S No.\"] = m2.group('sno')\n",
        "        parsed[\"Item No\"] = m2.group('itemno')\n",
        "        parsed[\"Description of Item\"] = m2.group('desc').strip()\n",
        "        parsed[\"Amount\"] = to_number(m2.group('amount'))\n",
        "        return parsed\n",
        "\n",
        "    # 4) Last resort: whole line as description\n",
        "    parsed[\"Description of Item\"] = line\n",
        "    return parsed\n",
        "\n",
        "# Assemble final item-breakup DataFrame from parsed rows or raw item-line strings\n",
        "def build_item_breakup(lines):\n",
        "    \"\"\"\n",
        "    lines: iterable of either dicts (already parsed) or raw strings (item lines)\n",
        "    Returns: pandas DataFrame with columns:\n",
        "      [\"S No.\", \"Item No\", \"Description of Item\", \"Unit\", \"Qty\", \"Rate\", \"Amount\"]\n",
        "    Numeric columns coerced to floats where possible.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for entry in lines:\n",
        "        if entry is None:\n",
        "            continue\n",
        "        if isinstance(entry, dict):\n",
        "            rows.append(entry)\n",
        "        else:\n",
        "            parsed = parse_item_line(entry)\n",
        "            if parsed:\n",
        "                rows.append(parsed)\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"S No.\", \"Item No\", \"Description of Item\", \"Unit\", \"Qty\", \"Rate\", \"Amount\"])\n",
        "    for c in [\"Qty\", \"Rate\", \"Amount\"]:\n",
        "        df[c] = df[c].apply(lambda v: to_number(v))\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# Safe helper to try-normalizing parsed tables and logging failures without halting\n",
        "def safe_normalize_and_parse(table_like, fallback_text_column=None):\n",
        "    \"\"\"\n",
        "    table_like: DataFrame or list-of-strings. If DataFrame, will attempt to find a column that appears to be lines.\n",
        "    fallback_text_column: explicit column name to use if DataFrame has multiple columns and one contains raw lines.\n",
        "    Returns: tuple(parsed_df, logs) where parsed_df is the assembled DataFrame and logs is a list of (reason, context).\n",
        "    \"\"\"\n",
        "    logs = []\n",
        "    lines = []\n",
        "\n",
        "    if isinstance(table_like, pd.DataFrame):\n",
        "        df = normalize_table_df(table_like)\n",
        "        if fallback_text_column and fallback_text_column in df.columns:\n",
        "            for v in df[fallback_text_column].astype(str).tolist():\n",
        "                lines.append(v)\n",
        "        else:\n",
        "            candidate_cols = list(df.columns)\n",
        "            chosen = None\n",
        "            max_score = -1\n",
        "            for col in candidate_cols:\n",
        "                col_sample = df[col].dropna().astype(str)\n",
        "                if col_sample.empty:\n",
        "                    continue\n",
        "                digit_frac = col_sample.str.contains(r'\\d').mean()\n",
        "                avg_len = col_sample.str.len().mean()\n",
        "                score = digit_frac * 2 + (avg_len / 100.0)\n",
        "                if score > max_score:\n",
        "                    max_score = score\n",
        "                    chosen = col\n",
        "            if chosen is None:\n",
        "                logs.append((\"no_candidate_column\", \"DataFrame empty or no suitable column found\"))\n",
        "            else:\n",
        "                for v in df[chosen].astype(str).tolist():\n",
        "                    lines.append(v)\n",
        "                logs.append((\"chosen_column\", chosen))\n",
        "    elif isinstance(table_like, (list, tuple, pd.Series, np.ndarray)):\n",
        "        for v in table_like:\n",
        "            lines.append(v)\n",
        "    else:\n",
        "        logs.append((\"unsupported_type\", str(type(table_like))))\n",
        "        return pd.DataFrame(columns=[\"S No.\", \"Item No\", \"Description of Item\", \"Unit\", \"Qty\", \"Rate\", \"Amount\"]), logs\n",
        "\n",
        "    parsed_df = build_item_breakup(lines)\n",
        "    return parsed_df, logs\n",
        "\n",
        "# Example usage (for notebook):\n",
        "# parsed_df, logs = safe_normalize_and_parse(raw_table_df, fallback_text_column='item_line')\n",
        "# parsed_df.to_csv('improved_item_breakup.csv', index=False)"
      ],
      "metadata": {
        "id": "r9GoG3Oqzwff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_b1tJDB0t-1T",
        "outputId": "662bca42-633f-4490-c5c4-9b755b6ada51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your tender PDF (single file). The uploaded file will appear as a variable `uploaded`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-36b92a15-770d-467b-92cd-1729159c76f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-36b92a15-770d-467b-92cd-1729159c76f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NIT-BCT-24-25-257.pdf to NIT-BCT-24-25-257 (1).pdf\n",
            "Uploaded file: NIT-BCT-24-25-257 (1).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing pages:   0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "Parsing pages:  48%|████▊     | 16/33 [00:00<00:00, 147.79it/s]/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "Parsing pages:  94%|█████████▍| 31/33 [00:00<00:00, 143.86it/s]/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:404: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-266906523.py:612: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  combined[col] = combined[col].astype(str).apply(lambda s: to_number(s))\n",
            "/tmp/ipython-input-266906523.py:778: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  for desc, qty in zip(df['Description of Item'].fillna(''), df['Qty'].fillna(0)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# TENDER BCT-24-25-257 | storage tank under the jurisdi | ₹42,145,189.36\n\n\n## NIT HEADER\n| Field | Value |\n|---|---|\n| Tender No | BCT-24-25-257 |\n| Name of Work | storage tank under the jurisdiction of Sr. DEN/North/MMCT. |\n| Bidding type | Normal Tender |\n| Tender Type | Open Bidding System Single Packet System |\n| Advertised Value | 42145189.36 |\n| Earnest Money | 360700.00 |\n| Period of Completion | 18 Months\nContract Type Works |\n| Are JV allowed to bid | No |\n| Tendering Section | CETR/N/II |\n| Bidding Start Date | 21/01/2025\nNumber of JV Member\nAre JV allowed to bid No 0\nAllowed\nAre Consortium allowed Number of Consortium\nNo 0\nto bid Member Allowed\nRanking Order For Bids Lowest to Highest Expenditure Type Capital |\n| Pre-Bid Conference | Pre-Bid Conference Date |\n| first_lines_sample | ['MUMBAI CENTRAL DIVISION-ENGINEERING/WESTERN RLY', 'TENDER DOCUMENT', 'Tender No: BCT-24-25-257 Closing Date/Time: 04/02/2025 15:00', 'Divisional Railway Manager Works Mumbai Central acting for and on behalf of The President of India invites E-', 'Tenders against Tender No BCT-24-25-257 Closing Date/Time 04/02/2025 15:00 Hrs. Bidders will be able to submit', 'their original/revised bids upto closing date and time only. Manual offers are not allowed against this tender, and any', 'such manual offer received shall be ignored.', '1. NIT HEADER'] |\n\n\n## SCHEDULE SUMMARY\n| raw                                                                       |\n|:--------------------------------------------------------------------------|\n| Description:- Schedule A1 (2.0 Earthwork)                                 |\n| Description:- Schedule A2 (4.0 Concrete work)                             |\n| Description:- Schedule A3 (5.0 R.C.C work)                                |\n| Description:- Schedule A4 (6.0 Masonary work)                             |\n| Description:- Schedule A5 (8.0 Cladding work)                             |\n| Description:- Schedule A6 (9.0 Wood and P.V.C work)                       |\n| Description:- Schedule A7 (10.0 Steel work)                               |\n| Description:- Schedule A8 (11.0 Flooring work)                            |\n| Description:- Schedule A9 (12.0 Roofing work)                             |\n| Description:- Schedule A10 (13.0 Finishing work)                          |\n| Description:- Schedule A11 (14.0 Repairs to building)                     |\n| Description:- Schedule A12 (15.0 Dismantling and Demolishing)             |\n| Description:- Schedule A13 (16.0 Road work)                               |\n| Description:- Schedule A14 (18.0 Water Supply)                            |\n| Description:- Schedule A15 (19.0 Drainage)                                |\n| Description:- Schedule A16 (21.0 Aluminum work)                           |\n| Description:- Schedule A17 (22.0 Water proofing)                          |\n| Description:- Schedule A18 (23.0 Rain water harvesting)                   |\n| Description:- All USSOR-2021 items                                        |\n| Description:- Hand packed dry rubble soling                               |\n| Description:- Supply and fixing signage board                             |\n| Description:- Providing and laying 35 mm thick heavy duty chequered tiles |\n| Description:- Supply & Fixing brass name plate                            |\n\n\n## TOP 10 COST DRIVERS (Global)\n_No cost drivers computed._\n\n\n## ELIGIBILITY CRITERIA\n- 4. ELIGIBILITY CONDITIONS Standard Financial Criteria S.No. Description ConfirmationRemarks Documents Required Allowed Uploading Financial Eligibility Criteria: The tenderer must have minimum average annual contractual turnover of[V/N or 'V' which ever is less; where V = Advertised value of the tender in crores of Rupees N= Number of years prescribed for completion of work for which bids have been invited. The average annual contractual turnover shall be calculated as an average of \"total contractual payments'' in the previous three financial years, as per the audited balance sheet. However, in case balance sheet of the Allowed 1 previous year is yet to be prepared/ audited, the audited balance sheet No No (Mandatory) of the fourth previous year shall be considered for calculating average annual contractual turnover. The tenderers shall submit requisite information as per Annexure-VIB, along with copies of Audited Balance Sheets duly certified by the Chartered Accountant/ Certificate from Chartered Accountant duly supported by Audited Balance Sheet. (Page- 14, Para 10.2, Part-I of GCC April 2022 and as per Advance Correction slip No 1 dt 14.07.2022) Standard Technical Criteria S.No. Description ConfirmationRemarks Documents Required Allowed Uploading Page 16 of 33 Run Date/Time: 09/01/2025 10:19:22\n- Standard Financial Criteria S.No. Description ConfirmationRemarks Documents Required Allowed Uploading Financial Eligibility Criteria: The tenderer must have minimum average annual contractual turnover of[V/N or 'V' which ever is less; where V = Advertised value of the tender in crores of Rupees N= Number of years prescribed for completion of work for which bids have been invited. The average annual contractual turnover shall be calculated as an average of \"total contractual payments'' in the previous three financial years, as per the audited balance sheet. However, in case balance sheet of the Allowed 1 previous year is yet to be prepared/ audited, the audited balance sheet No No (Mandatory) of the fourth previous year shall be considered for calculating average annual contractual turnover. The tenderers shall submit requisite information as per Annexure-VIB, along with copies of Audited Balance Sheets duly certified by the Chartered Accountant/ Certificate from Chartered Accountant duly supported by Audited Balance Sheet. (Page- 14, Para 10.2, Part-I of GCC April 2022 and as per Advance Correction slip No 1 dt 14.07.2022) Standard Technical Criteria S.No. Description ConfirmationRemarks Documents Required Allowed Uploading Page 16 of 33 Run Date/Time: 09/01/2025 10:19:22\n- Financial Eligibility Criteria: The tenderer must have minimum average annual contractual turnover of[V/N or 'V' which ever is less; where V = Advertised value of the tender in crores of Rupees N= Number of years prescribed for completion of work for which bids have been invited. The average annual contractual turnover shall be calculated as an average of \"total contractual payments'' in the previous three financial years, as per the audited balance sheet. However, in case balance sheet of the Allowed 1 previous year is yet to be prepared/ audited, the audited balance sheet No No (Mandatory) of the fourth previous year shall be considered for calculating average annual contractual turnover. The tenderers shall submit requisite information as per Annexure-VIB, along with copies of Audited Balance Sheets duly certified by the Chartered Accountant/ Certificate from Chartered Accountant duly supported by Audited Balance Sheet. (Page- 14, Para 10.2, Part-I of GCC April 2022 and as per Advance Correction slip No 1 dt 14.07.2022) Standard Technical Criteria S.No. Description ConfirmationRemarks Documents Required Allowed Uploading Page 16 of 33 Run Date/Time: 09/01/2025 10:19:22\n- a) The tenderer must have successfully completed or substantially completed any one of the following categories of work(s) during last 07 (seven) years, ending last day of month previous to the one in which tender is invited: (i) Three similar works each costing not less than the amount equal to 30% of advertised value of the tender, or (ii) Two similar works each costing not less than the amount equal to 40% of advertised value of the tender, or (iii) One similar work costing not less than the amount equal to 60% of advertised value of the tender. (As per Page- 12,13, Para 10.1, Part-I of GCC April 2022) (a)(i)For Works without composite components The technical eligibility for the work as per para Allowed 1 No No\n- composite components The technical eligibility for the work as per para Allowed 1 No No\n- values shall also be considered for fulfillment of technical eligibility criteria for different components. (As per Page-13, Para 10.1.b(1) Part-I of GCC April 2022. (b-1) For works with composite components The technical eligibility for major component of work as per para 10.1 above, shall be satisfied by either the 'JV in its own name & style' or 'Lead member of the JV' and technical eligibility for other component(s) of\n- technical eligibility for major component of work as per para 10.1 above, shall be satisfied by either the 'JV in its own name & style' or 'Lead member of the JV' and technical eligibility for other component(s) of\n- member of the JV' and technical eligibility for other component(s) of\n- technical eligibility criteria Note for Para I 7. I 5. I: a)a) The Major component of the work for this purpose shall be the component of work having highest value. In cases where value of two or more component of work is same, any one work can be classified as Major component of work. b) Value of a completed work done by a Member in an earlier JV shall be reckoned only to the extent of the concerned member's share in that JV for the purpose of satisfying his/her compliance to the above mentioned technical eligibility criteria in the tender under consideration. (Page-24, Para 17.15.1 Part-I of GCC April 2022 and As per Advance Correction slip No 1 dt 14.07.2022). (b)(2) In such cases, what constitutes a component in a composite work shall be clearly pre-defined with estimated tender cost of it, as part of\n- mentioned technical eligibility criteria in the tender under consideration. (Page-24, Para 17.15.1 Part-I of GCC April 2022 and As per Advance Correction slip No 1 dt 14.07.2022). (b)(2) In such cases, what constitutes a component in a composite work shall be clearly pre-defined with estimated tender cost of it, as part of\n- 1.2 No No Not Allowed the tender documents without any ambiguity. (As per Page-13, Para\n- 10.1.b(2) Part-I of GCC April 2022) Page 17 of 33 Run Date/Time: 09/01/2025 10:19:22\n- (b) (3) To evaluate the technical eligibility of tenderer, only components of work as stipulated in tender documents for evaluation of technical eligibility, shall be considered. The scope of work covered in other remaining components shall be either executed by tenderer himself if he has work experience as mentioned in clause 7 of the Standard General Conditions of Contractor through subcontractor fulfilling the requirements as per clause 7 of the Standard General Conditions of Contract or jointly i.e., partly himself and remaining through subcontractor, with prior approval of Chief Engineer in writing. However, if required in tender documents by way of Special Conditions, a formal agreement duly notarized, legally enforceable in the court of law, shall be executed by the main contractor with the subcontractor for the component(s) of work proposed to be executed by the subcontractor(s), and shall be submitted along with the offer for considering subletting of\n- eligibility, shall be considered. The scope of work covered in other remaining components shall be either executed by tenderer himself if he has work experience as mentioned in clause 7 of the Standard General Conditions of Contractor through subcontractor fulfilling the requirements as per clause 7 of the Standard General Conditions of Contract or jointly i.e., partly himself and remaining through subcontractor, with prior approval of Chief Engineer in writing. However, if required in tender documents by way of Special Conditions, a formal agreement duly notarized, legally enforceable in the court of law, shall be executed by the main contractor with the subcontractor for the component(s) of work proposed to be executed by the subcontractor(s), and shall be submitted along with the offer for considering subletting of\n- 1.3 No No Not Allowed that scope of work towards fulfilment of technical eligibility. Such subcontractor must fulfill technical eligibility criteria as follows: The subcontractor shall have successfully completed at least one work similar to work proposed for subcontract, costing not less than 35% value of work to be subletted, in last 5 years, ending last day of month previous to the one in which tender is invited through a works contract. Note: for subletting of work costing up to Rs 50 lakh, no previous work experience of subcontractor shall be asked for by the Railway. In case after award of contract or during execution of work it becomes necessary for contractor to change subcontractor, the same shall be done with subcontractor(s) fulfilling the requirements as per clause 7 of the Standard General Conditions of Contract, with prior approval of Chief Engineer in writing. (As per Page-13,14, Para 10.1.b(3) Part-I of GCC April 2022) If a bidder has successfully completed a work as subcontractor and the work experience certificate has been issued for such work to subcontractor by a Govt. Organization or public listed company as Allowed\n- that scope of work towards fulfilment of technical eligibility. Such subcontractor must fulfill technical eligibility criteria as follows: The subcontractor shall have successfully completed at least one work similar to work proposed for subcontract, costing not less than 35% value of work to be subletted, in last 5 years, ending last day of month previous to the one in which tender is invited through a works contract. Note: for subletting of work costing up to Rs 50 lakh, no previous work experience of subcontractor shall be asked for by the Railway. In case after award of contract or during execution of work it becomes necessary for contractor to change subcontractor, the same shall be done with subcontractor(s) fulfilling the requirements as per clause 7 of the Standard General Conditions of Contract, with prior approval of Chief Engineer in writing. (As per Page-13,14, Para 10.1.b(3) Part-I of GCC April 2022) If a bidder has successfully completed a work as subcontractor and the work experience certificate has been issued for such work to subcontractor by a Govt. Organization or public listed company as Allowed\n- subcontractor must fulfill technical eligibility criteria as follows: The subcontractor shall have successfully completed at least one work similar to work proposed for subcontract, costing not less than 35% value of work to be subletted, in last 5 years, ending last day of month previous to the one in which tender is invited through a works contract. Note: for subletting of work costing up to Rs 50 lakh, no previous work experience of subcontractor shall be asked for by the Railway. In case after award of contract or during execution of work it becomes necessary for contractor to change subcontractor, the same shall be done with subcontractor(s) fulfilling the requirements as per clause 7 of the Standard General Conditions of Contract, with prior approval of Chief Engineer in writing. (As per Page-13,14, Para 10.1.b(3) Part-I of GCC April 2022) If a bidder has successfully completed a work as subcontractor and the work experience certificate has been issued for such work to subcontractor by a Govt. Organization or public listed company as Allowed\n- 1.3.1 No No defined in Note for Item 10.1 Part-1 of GCC, the same shall be considered (Mandatory) for the purpose of fulfillment of credentials. (As per Page-15, Para 10.5.5, Part-I of GCC April 2022) Page 18 of 33 Run Date/Time: 09/01/2025 10:19:22\n- has been found eligible in other eligibility criteria/tender requirement. (As per Annexure-VI, Page-35, 36, Part-I of GCC April 2022) No Technical and Financial credentials are required for tenders having\n- 1.4.1 value up to Rs 50 lakh. (As per Clause 10.4, Page-14, Part-I of GCC April No No Not Allowed 2022) Work experience certificate from private individual shall not be considered. However, in addition to work experience certificates issued by any Govt. Organisation, work experience certificate issued by Public listed company having average annual turnover of Rs 500 crore and above in last 3 financial years excluding the current financial year, listed on National Stock Exchange or Bombay Stock Exchange, incorporated/registered at least 5 years prior to the date of closing of tender, shall also be considered provided the work experience certificate Allowed\n- 1.5 has been issued by a person authorized by the Public listed company to No No (Mandatory) issue such certificates. In case tenderer submits work experience certificate issued by public listed company, the tenderer shall also submit along with work experience certificate, the relevant copy of work order, bill of quantities, bill wise details of payment received duly certified by Chartered Accountant, TDS certificates for all payments received and copy of final/last bill paid by company in support of above work experience certificate. (As per Note for Item 10.1, Page-14 of GCC April 2022)\n- 1.6 Defination of Similar Work :- Construction of RCC overhead tank. No No Not Allowed Bidders shall confirm and certify on the behalf of the tenderer including its constituents as under: Page 19 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 5. COMPLIANCE Commercial-Compliance S.No. Description Confirmation Remarks Documents Required Allowed Uploading Please enter the percentage of local content in the material being offered. Please enter 0 for fully imported items, and 100 Allowed 1 for fully indigenous items. The definition and calculation of localNo Yes (Optional) content shall be in accordance with the Make in India policy as incorporated in the tender conditions. Page 20 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 2.1 Bid Security as mentioned in tender documents, failing whichYes No Not Allowed the tender shall be summarily rejected. (As per Clause 6 (a) Annex -I, Page-11, Part-I of GCC April 2022) Note: ii) Any firm recognized by Department of Industrial Policy and Promotion (DIPP as 'Startups' shall be exempted from payment of Bid Security subject to submission of Registration Allowed\n- 2.1.1 Certificate issued by appropriate authority. iii) Labour Yes Yes (Mandatory) Cooperative Societies shall submit only 50% of Bid Security shall also additionally submit Registration Certificate. (As per Clause 5 , Page-5, Part-I of GCC April 2022) General Instructions S.No. Description Confirmation Remarks Documents Required Allowed Uploading 1 Applicability of rules for this tender No No Not Allowed\n- 1.1 1.GCC-April 2022 with upto date correction slips No No Not Allowed\n- 1. USSOR and IR UNIFIED Standard Specification with upto date\n- 1.2 correction slip. 2. DSR with upto date correction slip (As per No No Not Allowed Clause 1.1(k), Page-42, Part-I of GCC April 2022)\n- 1.3 Relevant IS-CODE & RAILWAY CODES AND MANUALS. No No Not Allowed \"If any dispute arises between the parties with respect to this agreement any application or suit shall be instituted only in the court with the local lines or whose jurisdiction, the Western\n- 1.4 No No Not Allowed Railway's Divisional Headquarters office is Situated and both the parties shall be bound by this clause. (Headquarters letter no. CE-Circular No. 11/No. W/623/5/ARB/1 dt. 26.04.04) In these Special Conditions of Contract the following terms shall 2 have the meaning hereby assigned to them except where theNo No Not Allowed context otherwise requires. Page 21 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 2.1 of Rates) and CPWD Specifications-2019 (or latest) shall be usedNo No Not Allowed for all works related to Building Works, Road Works and Horticulture works and other Miscellaneous works with effect from 01.06.2021 with latest Correction Slips issued from time to time by CPWD. (b)Standard Specifications shall mean \"Specifications for Materials and Works of the Railway as specified under the authority of the Ministry of Railways or Chief Engineer or as\n- 2.2 No No Not Allowed amplified, added to or superseded by special specifications if any, appended to the Tender Forms. (GCC April 2022 Part I para I Instructions to tenderers (ITT)). (c) Standard Schedule of Rates (SSOR) shall mean the schedule of Rates adopted by the Railway which includes- 1.\"Unified Standard Schedule of Rates of the Railway (USSOR)\" i.e. the Standard Schedule of Rates of the Railway issued under the authority of the Chief Engineer from time to time, updated with correction slips issued up to date of inviting tender or as otherwise specified in the tender documents; 2. \"Delhi Schedule\n- 2.3 No No Not Allowed Of Rates (DSR)\" i.e. the Standard Schedule of Rates published by Director General / Central Public Works Department, Government of India, New Delhi, as adopted and modified by the Railway under the authority of the Chief Engineer from time to time, updated with correction slips issued up to date of inviting tender or as otherwise specified in the tender documents. (As per Clause 1.1(k), Page-42, Part-I of GCC April 2022) Where there is any conflict in conditions/Specifications 3 contained in various parts, order of precedence will be as givenNo No Not Allowed below i . Any foot note given by the Railway in the schedule of\n- 3.1 No No Not Allowed quantities and rates.\n- 3.2 ii. Description of item in the Schedule of Quantities and rates. No No Not Allowed\n- 3.3 iii. Special Specifications. No No Not Allowed\n- 3.4 iv Additional Special Conditions/of Contract. No No Not Allowed\n- 3.5 v. Standard Specifications. No No Not Allowed vi. Special Conditions of Contract & General Conditions of\n- 3.6 No No Not Allowed Contract April 2022 corrected upto date. 4 Signature on Receipts for Amount. No No Not Allowed Every receipt for money which may become payable or for any security which may become transferable to the Contractors under these presents, shall, if signed in the partnership name by anyone of the partners of a Contractor's firm be a good and sufficient discharge to the Railway in respect of the moneys or security purported to be acknowledged thereby and in the event of death of any of the Contractor, partners during the pendency of the contract, it is hereby expressly agreed that every receipt by anyone of the surviving Contractor partners shall if so signed\n- 4.1 as aforesaid be good and sufficient discharge as aforesaid No No Not Allowed provided that nothing in this Clause contained shall be deemed to prejudice or effect any claim which the Railway may hereafter have against the legal representative of any Contractor partner so dying for or in respect to any breach of any of the conditions of the contract, provided also that nothing in this clause contained shall be deemed to prejudice or effect the respective rights or obligations of the Contractor partners and of the legal representatives of any deceased Contractor partners interse. (As per Clause 53, Part-II, Page-86 of GCC-April 2022) Modification to GCC for introduction of measurement and recording of \"Executed works\" by the contractor in Railway construction works. \" Introduction of Item (As per clause 45 (i) 5 No No Not Allowed (a) (b), 45 (ii) (a) (b), 46.(1), 46.(2), 46.(3), 46.(4), 46A., 46 (A.1 to A.10), 51.(1), 51.(2), 51-A, Page- Part II of GCC-April 2022) Page-65 to 85 Page 22 of 33 Run Date/Time: 09/01/2025 10:19:22\n- v)Proper laying of the cables vi) No temporary joints to be permitted. vii) Use of proper size plug / sockets. For the un- metered connections of less than 1200 watt, only item No. VI & 7 No No Not Allowed VII with the use of 3 core cable with earth wire only to be insisted as other items will not be applicable. Before connecting the assets to electrical power supply, SSE incharge must personally be satisfied that the firm's installation is safe against any fire hazards/electric shocks. (This is as per letter no. Sr.DEE(P) BCT's letter no. EL.197/13/9 (Tech circular) dt 25/07/2018). (As per Para 31.4 (a)(b), Page-58 Part-II of GCC April 2022) Page 23 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 14.07.2022) Provisions of Contract Labour (Regulation and Abolition) Act, 1970 (Shramik Kalyan) (As per Para 55-A.1 to 55-A.5, 55-B, 55-C 10 No No Not Allowed (i) (a to e), 55-C (ii) & 55-D, Page- 88-89, Part-II of GCC-April 2022) Special Conditions S.No. Description Confirmation Remarks Documents Required Allowed Uploading Page 25 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 2.1 No No Not Allowed following Qualified Engineers during execution of the allotted work: (a) One Qualified Graduate Engineer when cost of work to be executed is Rs. 200 lakh and above, and (b)One Qualified Diploma Holder Engineer when cost of work to be executed is more than Rs. 25 lakh, but less than Rs. 200 lakh. (As per Railway Board's letter No. 2012/CE-I/CT/O/20 dtd. 10.05.2013) (B) Further, in case the contractor fails to employ the Qualified Engineer, as aforesaid in Para (A) above, he, in terms of provisions of Clause 26A.2 of the General Conditions of Contract, shall be liable to pay an amount of Rs. 40,000 and Rs. 25,000 for\n- 2.2 No No Not Allowed each month or part thereof for the default period for the provisions, as contained in Para A (a) and A (b) above respectively. (As per Railway Board's letter No. 2012/CE- I/CT/O/20 dtd. 10.05.2013) C) Provision for deployment of Qualified Engineers (Graduate Engineer or Diploma Holder Engineer) shall be for the values as prescribed above. However, for the works contract tenders, if it is considered appropriate by the tender inviting authority, not to\n- C) Provision for deployment of Qualified Engineers (Graduate Engineer or Diploma Holder Engineer) shall be for the values as prescribed above. However, for the works contract tenders, if it is considered appropriate by the tender inviting authority, not to\n- 2.3 have the services of qualified engineer, the same shall be soNo No Not Allowed mentioned in the tender documents by the concerned Executive with the approval of Officer not below the level of SAG Officer, for reasons to be recorded in writing. (As per Railway Board's letter No. 2012/CEI/CT/O/20 dtd. 10.05.2013) As per para 26A.3 No. of qualified Engineers required to be deployed by the Contractor for various activities contained in\n- 2.4 the works contract shall be specified in the tender documents as No No Not Allowed 'Special Condition of Contract' by the tender inviting authority.\" (As per Clause 26A.3 Page-57, Part-II of GCC April 2022) RESTRICTIONS ON ARBITRATION CLAUSES (As per Clause 64.(1), 3 No No Not Allowed Page-97, 98, Part-II of GCC April 2022) Demand for Arbitration: (As per Clause 64.(1), Page-97, 98, Part-\n- 3.1 No No Not Allowed II of GCC April 2022) Settlement of disputes-Indian railway Arbitration and\n- 3.2 Conciliation Rules: (As per Clause No 63, Page-95-97, Part-II of No No Not Allowed GCC April 2022) These special conditions shall prevail over existing clauses 63\n- 3.3 No No Not Allowed and 64 of General Conditions of Contract April 2022. GUIDELINE FOR THE MAINTENANCE PERIOD (As per Clause 47, 4 No No Not Allowed Page-82, Part-II of GCC April 2022) Page 26 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 4.1 arise in or be discovered or be in any way connected with the No No Not Allowed works, provided that such damage or defect is not directly caused by errors in the contract documents, act of providence or insurrection or civil riot, and the Contractor shall be liable for and shall pay and make good to the Railway or other persons legally entitled thereto whenever required by the Engineer so to do, all losses, damages, costs and expenses they or any of them may incur or be put or be liable to by reasons or in consequence of the operations of the Contractor or of his failure in any respect. (As per Clause No. 47, Page-82, Part-II of GCC April 2022) However, for a zonal work, the maintenance period shall be as a) Repair and maintenance work including white/color washing: three calendar months from date of completion. b) All new works\n- 4.2 No No Not Allowed except earth work: Six calendar months from date of completion. (As per Annexure-III, Page-30,31, Part-I of GCC April 2022) To cover up monsoon period, the maintenance period will be extended in cases when required and contractor shall remain responsible for maintenance for this extended period also. The contractor shall make good and remedy at his own expense within such period as may be stipulated by the Engineer, any defect which may develop or may be before the expiry of this period and intimation of which has been sent to the contractor within seven days of the expiry of the said period by a letter,\n- 4.3 No No Not Allowed sent by hand delivery or by registered post. In case the contractor fails to make adequate arrangements to rectify the defects within seven days of the receipt of such notices, the Engineer without further notice may make his own arrangement to rectify the defects and the cost of such rectification shall be recovered from the Security Deposit of the contractor or from any other money due to the contractor under this or any other contract. 5 SPECIAL CONDITION FOR TAX DEDUCTION No No Not Allowed (1) In respect of works, the contract value of which is more than Rs.5,000/- each, a deduction of 2% on the gross payment from each of the Contractor's bills shall be made in terms of section\n- 5.1 No No Not Allowed 194(C) of the Income Tax Act of 1961 & 1991. (From time to time surcharge will also be deducted along with I. Tax as per extent rules. (2)The Railway will deduct GST if leviable in a particular state where the work is going on, the gross amount of each bill while\n- 5.2 making payment to the contractor(s). The recovery shall beNo No Not Allowed governed as per the guide lines & rates prescribed by the concerned State Government. 3) Any Other taxes The Contractor shall bear in full all taxes and royalties levied by the State Government and/or Central Government from time to time. This would be entirely a matter\n- 5.3 between the contractor and State Government/or CentralNo No Not Allowed Government. Railway will recover the taxes and royalties through final bills if the contractor fails to pay the taxes and royalties to the Government. DETAILS OF INSPECTION REGISTER AND RECORDS ARE TO 6 No No Not Allowed\n- 6.1 No No Not Allowed reasonable times. Records of tests made shall be handed over to the Engineer/s representative after carrying out the tests. The following registers will be maintained at site by the Contractor/s. 2) Site Order Register: The Contractor/s shall promptly acknowledge by putting his signature in the site order against any order given therein by the Engineer or his representative or\n- 6.2 No No Not Allowed his superior officers and comply with them. The Compliance shall be reported by the Contractor/s to the Engineer in good time so that it can be checked. 3) Labour Register: This register will be maintained to show daily\n- 6.3 strength of labour in different categories employed by the No No Not Allowed Contractor/s. 4) LOG book of events: All events are required to be\n- 6.4 No No Not Allowed chronologically logged in this book shift wise and date wise. 5)Cement & steel registers shall be maintained by the\n- 6.5 No No Not Allowed contractor. APPLICABILITY OF PRICE VARIATION CLAUSE AS PER GCC 7 No No Not Allowed\n- 14.07.2022).@10'46@(1) Rates for Extra Item(s) of Works: (a) Standard Schedule of Rates (SSOR) Items: Any item of work carried out by the Contractor on the instructions of the Engineer which is not included in the accepted Bil(s) of Quantities but figures in the Standard Schedule of Rates (SSOR), shall be executed at the rates set forth in the \"Standard Schedule of Rates (SSOR)\" modified by the tender percentage as accepted in the contract for that chapter of Standard Schedule of Rates (SSOR). For item(s) not covered in this sub clause, the rate shall\n- 7.2 (As per Railway Boards letter No. 2013/CE/I/CT/O/10-PVC-Pt.I No No Not Allowed dtd.27.01.2015) b. As per PCE/CCG letter No.W118/0 Vol. II (W6) date 14.06.2019. The security deposit against the contract shall be released only after the contractor has submitted the final PVC bill wherever applicable. 8 DISASTER MANAGMENT No No Not Allowed \"All the available vehicles and equipment's of the contractor can be drafted by the Railway Administration in case of accidents/natural calamities involving human lives. The payment for such drafting shall be made according to the rates as shall be fixed by the Engineer. However, if the contractor is not satisfied\n- 8.1 with the decision of the Engineer in this respect he may appeal No No Not Allowed to the Chief Engineer within 30 Days of getting the decision of the Engineer, supported by analysis of the rates claimed. The Chief Engineer's decision after hearing both the parties in the matter would be final and binding on the contractor and the Railway\". 9 EMERGENCY WORK No No Not Allowed In the event of any accident or failure occurring in the execution of work/ arising out of it which in the opinion of the Engineer requires immediate attention, the Railway may bring its own workmen or other agency/agencies to execute or partly execute\n- 9.1 No No Not Allowed the necessary work or carry out repairs if the Engineer-in-charge considers that the contractor(s) is/are not in a position to do so in time without giving any notice and charge the cost thereof, to be determined by the Engineer-in-charge, to the contractor. 10 DAMAGE BY ACCIDENT/ FLOOD/ TIDES OR NATURAL CALAMITIES No No Not Allowed The Contractor shall take all precautions against damages from accidents, floods tides or other natural occurrences. He shall not be entitled to any compensation for his tools, plants, materials, machines and other equipment lost or damaged by any cause whatsoever. The Contractor shall be liable to make good the damage to any structure or part of a structure, plant or material\n- DIPP Relaxation of eligibility criteria for work tenders for 'Startups' (recognized by Department of Industrial Policy and Promotion, Ministry of Commerce and Industry) has been examined in Board's office and Board (ME & FC) have approved as under- \"The technical and financial eligibility criteria mentioned in GCC shall normally apply to all firms including 'Startup' firms (recognized by Department of Industrial Policy 18 and Promotion, Ministry of Commerce and Industry). However, No No Not Allowed before inviting tender, General Manager, on the recommendation of PHOD/CHOD of the department inviting the tender and associate finance, can relax the applicability of eligibility criteria to 'Startup' firms (recognized by Department of Industrial Policy and Promotion, Ministry of Commerce and Industry). on case-to-case basis. (As per Railway Board Letter No. 2012/CE-I/CT/O/5, dated. 24-04-2019.) 19 Special Condition of Drawings No No Not Allowed\n- as under- \"The technical and financial eligibility criteria mentioned in GCC shall normally apply to all firms including 'Startup' firms (recognized by Department of Industrial Policy 18 and Promotion, Ministry of Commerce and Industry). However, No No Not Allowed before inviting tender, General Manager, on the recommendation of PHOD/CHOD of the department inviting the tender and associate finance, can relax the applicability of eligibility criteria to 'Startup' firms (recognized by Department of Industrial Policy and Promotion, Ministry of Commerce and Industry). on case-to-case basis. (As per Railway Board Letter No. 2012/CE-I/CT/O/5, dated. 24-04-2019.) 19 Special Condition of Drawings No No Not Allowed\n- eligibility criteria to 'Startup' firms (recognized by Department of Industrial Policy and Promotion, Ministry of Commerce and Industry). on case-to-case basis. (As per Railway Board Letter No. 2012/CE-I/CT/O/5, dated. 24-04-2019.) 19 Special Condition of Drawings No No Not Allowed\n- 1. Contractor will provide architectural drawings with elevations etc. complete, structural design details/drawings, design calculations, GAD (i.e. foundation, beams, lintel, column, roof etc. based on railway approved GAD and will get it approved from railway. Contractor will also submit 3 copies with soft copy (2 nos.CD/pen drive) to railway for the same. 2. Design shall be followed for respective zones and should be resistant from various disasters occurs in respective zones. Design shall be proof checked by IIT / NIT/ Govt engineering college which will be further got approved from railway before actual commencement of work. 3. Contractor has to submit completion plan in 75 Micron double mate GARWARE or similar tracing film\n- 2.1 Services Within One Year of their Retirement. (As per clause No Yes No Not Allowed\n- 59.(9), Page-91, Part-II of GCC April 2022) Certificate of NO Relative being an employee of Western Railway Allowed\n- 2.2 No No as per attached Performa. (Mandatory) Joint Venture (JV) in works tenders (As per Clause No. 17.1 to Allowed 3 No No\n- 17.15.3, Page-20 to 25, Part-1 of GCC April 2022) (Mandatory) Participation of Partnership Firm in works tenders. (As per Clause Allowed 4 No No No. 18.1 to 18.11, Page-25 to 27, Part-1 of GCC April 2022) (Mandatory) Page 30 of 33 Run Date/Time: 09/01/2025 10:19:22\n- 6. Documents attached with tender S.No. Document Name Document Description 1 SPLCONDTECHOHTANKDRD.pdf SPECIAL CONDITION TECHINICAL ACS-2toGCC-2022_2022-CE-1-CT-GCC-2022- 2 GCC correction slip No. 2 dtd.13.12.2022 POLICY_13.12.2022_1.pdf 2022-CE-I-CT- Clarification regarding submission of 3 GCCCorrespondencedated.14.05.2024.pdf Annexure-V\n\n\n## RISK ANALYSIS\n• Timeline: 18 Months\nContract Type Works → check feasibility\n\n\n## FLAGS\n<span style='color:red; font-weight:bold'>JV NOT ALLOWED</span>\n<span style='color:red; font-weight:bold'>Single Packet System</span>\n<span style='color:red; font-weight:bold'>Earnest Money: ₹360,700</span>\n\n\n## PARSING LOGS & CONFIDENCE\n**Confidence scores:**\n- text: 98%\n- raw_text: 88%\n- nit_header: 96%\n- schedules_summary: 92%\n- item_breakups: 90%\n- grouping: 92%\n- eligibility: 90%\n- top10: 20%\n- risk: 70%\n- flags: 95%\n\n**Progress logs (highlights):**\n- Table parse error on page 23: 'DataFrame' object has no attribute 'str'\n- Table parse error on page 26: 'DataFrame' object has no attribute 'str'\n- Table parse error on page 27: 'DataFrame' object has no attribute 'str'\n- Table parse error on page 29: 'DataFrame' object has no attribute 'str'\n- Table parse error on page 30: 'DataFrame' object has no attribute 'str'\n- Parsed table on page 30 with pdfplumber -> schedule Item- 18 Schedule A18 (23.0 Rain water harvesting)\n- Table parse error on page 32: 'DataFrame' object has no attribute 'str'\n- Parsed table on page 32 with pdfplumber -> schedule Item- 18 Schedule A18 (23.0 Rain water harvesting)\n- Extracted item breakups for 11 schedules.\n- Grouped Schedule A entries by parent class using regex.\n- Extracted 74 eligibility bullets from pages 15-35.\n- Top10 computation error: Reindexing only valid with uniquely valued Index objects"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved top10.csv\n",
            "Saved full_breakup.json\n",
            "Saved parsed_data.pkl\n",
            "\n",
            "Export files available for download:\n",
            "- top10.csv\n",
            "- full_breakup.json\n",
            "- parsed_data.pkl\n",
            "\n",
            "Attempting to trigger downloads (browser will prompt)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-266906523.py:968: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
            "  ser[k] = df.fillna('').to_dict(orient='records')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d479568-5605-4064-b786-21562927790e\", \"top10.csv\", 1)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_53d5b010-b2d0-45be-88a2-fa47228b95ed\", \"full_breakup.json\", 136137)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4687f7a5-5099-4b44-84bb-daeff4864eb7\", \"parsed_data.pkl\", 226816)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PARSING SUMMARY ---\n",
            "File: NIT-BCT-24-25-257 (1).pdf\n",
            "Pages processed: 33\n",
            "Confidence scores:\n",
            " - text: 98%\n",
            " - raw_text: 88%\n",
            " - nit_header: 96%\n",
            " - schedules_summary: 92%\n",
            " - item_breakups: 90%\n",
            " - grouping: 92%\n",
            " - eligibility: 90%\n",
            " - top10: 20%\n",
            " - risk: 70%\n",
            " - flags: 95%\n",
            "\n",
            "Top logs:\n",
            " - Table parse error on page 27: 'DataFrame' object has no attribute 'str'\n",
            " - Table parse error on page 29: 'DataFrame' object has no attribute 'str'\n",
            " - Table parse error on page 30: 'DataFrame' object has no attribute 'str'\n",
            " - Parsed table on page 30 with pdfplumber -> schedule Item- 18 Schedule A18 (23.0 Rain water harvesting)\n",
            " - Table parse error on page 32: 'DataFrame' object has no attribute 'str'\n",
            " - Parsed table on page 32 with pdfplumber -> schedule Item- 18 Schedule A18 (23.0 Rain water harvesting)\n",
            " - Extracted item breakups for 11 schedules.\n",
            " - Grouped Schedule A entries by parent class using regex.\n",
            " - Extracted 74 eligibility bullets from pages 15-35.\n",
            " - Top10 computation error: Reindexing only valid with uniquely valued Index objects\n",
            "\n",
            "Partial counts (guaranteed):\n",
            " - NIT header fields: 12\n",
            " - Schedule summaries parsed: 23\n",
            " - Item breakup schedules: 11\n",
            " - Eligibility bullets: 74\n",
            "Saved parsed_summary.md\n",
            "\n",
            "Analysis complete. Use the downloaded files for further inspection.\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Notebook Script\n",
        "# STEP 1: Run → installs\n",
        "# STEP 2: Upload your tender PDF\n",
        "# STEP 3: Wait → Full analysis ready!\n",
        "# Single-cell runnable Colab script. Paste into a Colab code cell and execute.\n",
        "\n",
        "# ------------------------------\n",
        "# INSTALL DEPENDENCIES\n",
        "# ------------------------------\n",
        "!pip install -q pdfplumber tabula-py PyPDF2 pandas tqdm pytesseract pdf2image openpyxl\n",
        "\n",
        "# tabula-py requires Java installed on Colab (already present in Colab). If not, user will see warnings but code will continue.\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "import traceback\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from decimal import Decimal, InvalidOperation\n",
        "\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import PyPDF2\n",
        "try:\n",
        "    import tabula\n",
        "except Exception as e:\n",
        "    tabula = None\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Markdown, display, HTML\n",
        "\n",
        "# ------------------------------\n",
        "# UTILITIES\n",
        "# ------------------------------\n",
        "# Minimal helper functions for numeric parsing and safe operations.\n",
        "def to_number(s):\n",
        "    \"\"\"Try to parse numbers robustly, remove commas and currency symbols.\"\"\"\n",
        "    if s is None:\n",
        "        return None\n",
        "    if isinstance(s, (int, float, Decimal)):\n",
        "        return s\n",
        "    s = str(s).strip()\n",
        "    if s == '':\n",
        "        return None\n",
        "    s = s.replace('\\xa0', ' ')\n",
        "    # remove currency symbols and words\n",
        "    s = re.sub(r'[^\\d\\.\\-]', '', s)\n",
        "    try:\n",
        "        if s.count('.') > 1:\n",
        "            # If there are multiple dots, remove all but last\n",
        "            parts = s.split('.')\n",
        "            s = ''.join(parts[:-1]) + '.' + parts[-1]\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return int(s)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def safe_extract_group(m, idx=1):\n",
        "    try:\n",
        "        return m.group(idx).strip()\n",
        "    except Exception:\n",
        "        return ''\n",
        "\n",
        "def numeric_tokens_from_line(line):\n",
        "    \"\"\"Return last 4 numeric-looking tokens from a line as possible [Unit?, Qty, Rate, Amount]\"\"\"\n",
        "    tokens = re.split(r'\\s{2,}|\\t', line.strip())\n",
        "    # flatten tokens by splitting on spaces if necessary if too few tokens\n",
        "    if len(tokens) < 6:\n",
        "        tokens = re.split(r'\\s+', line.strip())\n",
        "    nums = []\n",
        "    for t in reversed(tokens):\n",
        "        v = to_number(t)\n",
        "        if v is not None:\n",
        "            nums.append((t, v))\n",
        "        if len(nums) >= 4:\n",
        "            break\n",
        "    nums.reverse()\n",
        "    return nums, tokens\n",
        "\n",
        "def confidence_msg(section, method, score):\n",
        "    return f\"Section {section} parsed with {method} – {score:.0f}% confidence\"\n",
        "\n",
        "def download_file(path):\n",
        "    try:\n",
        "        files.download(path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not download {path}: {e}\")\n",
        "\n",
        "# ------------------------------\n",
        "# UPLOAD STEP\n",
        "# ------------------------------\n",
        "print(\"Upload your tender PDF (single file). The uploaded file will appear as a variable `uploaded`.\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Re-run and upload a PDF.\")\n",
        "\n",
        "# pick first file\n",
        "uploaded_fname = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {uploaded_fname}\")\n",
        "\n",
        "# save path\n",
        "pdf_path = uploaded_fname\n",
        "\n",
        "# ------------------------------\n",
        "# PARSING ORCHESTRATOR\n",
        "# ------------------------------\n",
        "verbose = True\n",
        "progress_logs = []\n",
        "confidence_scores = {}\n",
        "parse_results = {\n",
        "    \"nit_header\": {},\n",
        "    \"schedules_summary\": None,\n",
        "    \"item_breakups\": {},\n",
        "    \"eligibility_criteria\": {\"bullets\": [], \"raw_text\": \"\"},\n",
        "    \"flags\": [],\n",
        "    \"top10\": None,\n",
        "    \"raw_text_pages\": []\n",
        "}\n",
        "\n",
        "# read raw PDF pages texts using PyPDF2 (fast)\n",
        "def method3_pypdf2_text(path):\n",
        "    texts = []\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for p in range(len(reader.pages)):\n",
        "                try:\n",
        "                    texts.append(reader.pages[p].extract_text() or \"\")\n",
        "                except Exception:\n",
        "                    texts.append(\"\")\n",
        "        return texts, True, \"Method 3 (PyPDF2)\"\n",
        "    except Exception as e:\n",
        "        return [], False, f\"Method 3 failed: {e}\"\n",
        "\n",
        "# method2: pdfplumber extract page-level text + tables\n",
        "def method2_pdfplumber(path):\n",
        "    pages_text = []\n",
        "    tables_by_page = {}\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                try:\n",
        "                    t = page.extract_text() or \"\"\n",
        "                except Exception:\n",
        "                    t = \"\"\n",
        "                pages_text.append(t)\n",
        "                # extract simple tables\n",
        "                try:\n",
        "                    page_tables = page.extract_tables()\n",
        "                    if page_tables:\n",
        "                        tables_by_page[i+1] = page_tables\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return pages_text, tables_by_page, True, \"Method 2 (pdfplumber)\"\n",
        "    except Exception as e:\n",
        "        return [], {}, False, f\"Method 2 failed: {e}\"\n",
        "\n",
        "# method1: tabula (attempt at high-quality table extraction)\n",
        "def method1_tabula(path):\n",
        "    all_tables = []\n",
        "    if tabula is None:\n",
        "        return [], False, \"tabula not available\"\n",
        "    try:\n",
        "        # try lattice first\n",
        "        try:\n",
        "            dfs = tabula.read_pdf(path, pages='all', lattice=True, pandas_options={'header': None})\n",
        "            if isinstance(dfs, list):\n",
        "                all_tables.extend(dfs)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # try stream as fallback\n",
        "        try:\n",
        "            dfs2 = tabula.read_pdf(path, pages='all', lattice=False, pandas_options={'header': None})\n",
        "            if isinstance(dfs2, list):\n",
        "                all_tables.extend(dfs2)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return all_tables, True, \"Method 1 (tabula)\"\n",
        "    except Exception as e:\n",
        "        return [], False, f\"Method 1 failed: {e}\"\n",
        "\n",
        "# Run the three methods in sequence with retries for robust coverage\n",
        "max_retries = 3\n",
        "method_outputs = {}\n",
        "# Method 1 attempts\n",
        "for attempt in range(1, max_retries+1):\n",
        "    try:\n",
        "        tabula_tables, ok_tabula, tabula_msg = method1_tabula(pdf_path)\n",
        "        method_outputs['tabula'] = {'ok': ok_tabula, 'msg': tabula_msg, 'tables': tabula_tables}\n",
        "        if ok_tabula and len(tabula_tables) > 0:\n",
        "            progress_logs.append(confidence_msg(\"Tables(All)\", \"Method 1 (tabula)\", 92))\n",
        "            confidence_scores['tables'] = 92\n",
        "            break\n",
        "    except Exception as e:\n",
        "        progress_logs.append(f\"Method1 attempt {attempt} failed: {e}\")\n",
        "        time.sleep(0.3)\n",
        "else:\n",
        "    progress_logs.append(\"Method1 (tabula) exhausted retries.\")\n",
        "\n",
        "# Method 2 attempts\n",
        "for attempt in range(1, max_retries+1):\n",
        "    pages_text, tables_by_page, ok_pdfplumber, pdfplumber_msg = method2_pdfplumber(pdf_path)\n",
        "    method_outputs['pdfplumber'] = {'ok': ok_pdfplumber, 'msg': pdfplumber_msg, 'pages_text': pages_text, 'tables_by_page': tables_by_page}\n",
        "    if ok_pdfplumber and len(pages_text) > 0:\n",
        "        progress_logs.append(confidence_msg(\"Text+Tables(All)\", \"Method 2 (pdfplumber)\", 98))\n",
        "        confidence_scores['text'] = 98\n",
        "        break\n",
        "    time.sleep(0.2)\n",
        "else:\n",
        "    progress_logs.append(\"Method2 (pdfplumber) exhausted retries.\")\n",
        "\n",
        "# Method 3 attempts\n",
        "for attempt in range(1, max_retries+1):\n",
        "    texts3, ok_p3, p3msg = method3_pypdf2_text(pdf_path)\n",
        "    method_outputs['pypdf2'] = {'ok': ok_p3, 'msg': p3msg, 'pages_text': texts3}\n",
        "    if ok_p3 and len(texts3) > 0:\n",
        "        progress_logs.append(confidence_msg(\"RawText(All)\", \"Method 3 (PyPDF2)\", 88))\n",
        "        confidence_scores['raw_text'] = 88\n",
        "        break\n",
        "    time.sleep(0.2)\n",
        "else:\n",
        "    progress_logs.append(\"Method3 (PyPDF2) exhausted retries.\")\n",
        "\n",
        "# Prefer pdfplumber text for downstream parsing (best formatting). Fallback to pypdf2.\n",
        "pages_text = method_outputs.get('pdfplumber', {}).get('pages_text') or method_outputs.get('pypdf2', {}).get('pages_text') or []\n",
        "parse_results['raw_text_pages'] = pages_text\n",
        "\n",
        "# Helper: merge multi-line table fragments into normalized lines\n",
        "def normalize_text_block(block):\n",
        "    # Remove repeated multiple spaces, keep newlines\n",
        "    return '\\n'.join([re.sub(r'[ \\t]{2,}', '  ', ln).rstrip() for ln in block.splitlines() if ln.strip()!=''])\n",
        "\n",
        "# ------------------------------\n",
        "# PARSE NIT HEADER (page 1)\n",
        "# ------------------------------\n",
        "# We'll attempt structured extraction from first page text using regexes.\n",
        "try:\n",
        "    header_page_text = pages_text[0] if len(pages_text) >= 1 else \"\"\n",
        "    header_text = header_page_text.replace('\\r','\\n')\n",
        "    parse_results['nit_header_raw'] = header_text\n",
        "\n",
        "    nit = {}\n",
        "    # Common fields with flexible regexes\n",
        "    patterns = {\n",
        "        'Tender No': r'Tender No[:\\s]*([A-Za-z0-9\\-\\_\\/]+)',\n",
        "        'Name of Work': r'Name of Work\\s*(.*?)\\n',\n",
        "        'Bidding type': r'Bidding type\\s*(.*?)\\n',\n",
        "        'Tender Type': r'Tender Type\\s*(.*?)\\n',\n",
        "        'Tender Closing Date': r'Tender Closing Date\\s*Time\\s*([0-9\\/\\:\\sA-Za-z]+)',\n",
        "        'Advertised Value': r'Advertised Value\\s*([\\d\\.,]+)',\n",
        "        'Earnest Money': r'Earnest Money \\(Rs\\.\\)\\s*([\\d\\.,]+)',\n",
        "        'Period of Completion': r'Period of Completion\\s*([0-9A-Za-z\\s]+)',\n",
        "        'Are JV allowed to bid': r'Are JV allowed to bid\\s*(Yes|No)',\n",
        "        'Tendering Section': r'Tendering Section\\s*(.*?)\\n',\n",
        "        'Bidding Start Date': r'Bidding Start Date\\s*([0-9\\/\\:\\sA-Za-z]+)',\n",
        "        'Pre-Bid Conference': r'Pre-Bid Conference\\s*(.*?)\\n'\n",
        "    }\n",
        "    for k, pat in patterns.items():\n",
        "        m = re.search(pat, header_text, re.IGNORECASE|re.DOTALL)\n",
        "        if m:\n",
        "            nit[k] = m.group(1).strip()\n",
        "    # fallback: extract lines like \"Tender No: BCT-24-25-257 Closing Date/Time 04/02/2025 15:00 Hrs\"\n",
        "    m2 = re.search(r'Tender No[:\\s]*([A-Za-z0-9\\-\\_]+)', header_text)\n",
        "    if m2 and 'Tender No' not in nit:\n",
        "        nit['Tender No'] = m2.group(1).strip()\n",
        "    # Advertised value may contain currency or other format\n",
        "    adv = nit.get('Advertised Value') or re.search(r'Advertised Value\\s*([^\\n]+)', header_text)\n",
        "    if adv:\n",
        "        nit['Advertised Value'] = re.sub(r'[^\\d\\.,]', '', adv if isinstance(adv, str) else adv.group(1))\n",
        "    # Try to get Title line preceding \"Tender Document\"\n",
        "    first_lines = [ln.strip() for ln in header_text.splitlines() if ln.strip()]\n",
        "    if first_lines:\n",
        "        # pick first non-empty large line\n",
        "        nit['first_lines_sample'] = first_lines[:8]\n",
        "    parse_results['nit_header'] = nit\n",
        "    progress_logs.append(\"NIT header extracted from page 1.\")\n",
        "    confidence_scores['nit_header'] = 96 if nit else 50\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"NIT header parse error: {e}\")\n",
        "    parse_results['nit_header'] = {}\n",
        "    confidence_scores['nit_header'] = 40\n",
        "\n",
        "# ------------------------------\n",
        "# PARSE SCHEDULE SUMMARY\n",
        "# ------------------------------\n",
        "# Try to extract the Schedule Summary table using tabula results first; otherwise parse from raw text.\n",
        "schedules_df = None\n",
        "try:\n",
        "    # If tabula produced many tables, find the table having header \"S.No. Item\" or \"Schedule\"\n",
        "    tabula_tables = method_outputs.get('tabula', {}).get('tables') or []\n",
        "    candidate = None\n",
        "    for df in tabula_tables:\n",
        "        # Convert to string and handle potential non-string columns before applying upper()\n",
        "        txt = ' '.join(df.astype(str).fillna('').apply(lambda r: ' '.join(r), axis=1).tolist()).upper()\n",
        "        if 'S.NO' in txt or 'S.No.' in txt or 'SCHEDULE' in txt:\n",
        "            candidate = df\n",
        "            break\n",
        "    if candidate is not None:\n",
        "        # try to clean candidate into a schedule summary df\n",
        "        df = candidate.copy()\n",
        "        df = df.replace(r'^\\s*$', pd.NA, regex=True)\n",
        "        schedules_df = df\n",
        "    else:\n",
        "        # fallback parsing from raw text: search for \"SCHEDULE\" block\n",
        "        full = '\\n'.join(pages_text[:6])  # schedule summary usually near start\n",
        "        # find \"S.No. Item\" header line\n",
        "        block = ''\n",
        "        m = re.search(r'2\\.\\s*SCHEDULE(.*?)3\\.\\s*ITEM BREAKUP', '\\n'.join(pages_text[:8]), re.IGNORECASE|re.DOTALL)\n",
        "        if m:\n",
        "            block = m.group(1)\n",
        "        else:\n",
        "            # take pages 0-2\n",
        "            block = '\\n'.join(pages_text[:4])\n",
        "        # attempt to extract lines with Description:- and Amounts\n",
        "        lines = [ln for ln in block.splitlines() if ln.strip()]\n",
        "        rows = []\n",
        "        for ln in lines:\n",
        "            if re.search(r'Description:-', ln, re.IGNORECASE):\n",
        "                # previous few lines contain value\n",
        "                rows.append(ln.strip())\n",
        "        # build a very small DF fallback\n",
        "        schedules_df = pd.DataFrame({'raw': rows})\n",
        "    parse_results['schedules_summary'] = schedules_df\n",
        "    confidence_scores['schedules_summary'] = 92 if schedules_df is not None else 45\n",
        "    progress_logs.append(\"Schedule summary extracted.\")\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"Schedule summary parse error: {e}\")\n",
        "    parse_results['schedules_summary'] = None\n",
        "    confidence_scores['schedules_summary'] = 40\n",
        "\n",
        "# ------------------------------\n",
        "# PARSE ITEM BREAKUPS (robust line-by-line parser)\n",
        "# ------------------------------\n",
        "# Strategy:\n",
        "#  - Use pdfplumber page tables where available (method 2) to extract row tables directly.\n",
        "#  - Else, parse page text using regex to find \"Item- \" blocks and read lines that look like item rows.\n",
        "#  - Group items by Schedule headings (e.g., \"Schedule A1 (2.0 Earthwork)\" or \"Schedule A3 (5.0 R.C.C work)\")\n",
        "item_breakups = defaultdict(list)\n",
        "item_tables_dfs = {}\n",
        "\n",
        "try:\n",
        "    # Gather page-level table fragments from pdfplumber\n",
        "    pdfpl_tables = method_outputs.get('pdfplumber', {}).get('tables_by_page', {}) or {}\n",
        "    # Use both pdfplumber tables and textual parsing\n",
        "    num_pages = len(pages_text)\n",
        "    schedule_title = None\n",
        "    current_schedule = None\n",
        "    buffer_rows = []\n",
        "    page_iter = range(num_pages)\n",
        "    # Pre-scan to find schedule headers and their page indices\n",
        "    schedule_page_map = {}\n",
        "    schedule_heading_re = re.compile(r'Schedule\\s+A[-\\d\\w\\(\\)\\.\\s]*|Schedule\\s+B[-\\d\\w\\(\\)\\.\\s]*|Schedule\\s+C[-\\d\\w\\(\\)\\.\\s]*|Schedule\\s+Schedule\\s+[A-C]', re.IGNORECASE)\n",
        "    # also look for \"Item- \" and \"Schedule A1 (2.0 Earthwork)\" patterns\n",
        "    schedule_pat2 = re.compile(r'Schedule\\s*(A[\\d\\w]*)\\s*\\(?([0-9\\.]+)?\\)?\\s*[-:]*\\s*(.*)', re.IGNORECASE)\n",
        "    parent_work_re = re.compile(r'(\\d+\\.\\d+)\\s+([A-Z\\s&\\.\\-]+(?:WORK|CONCRETE|STEEL|WATER|DRAINAGE|ROOFING|MASONRY|PLASTER|CLADDING|FINISHING)?)', re.IGNORECASE)\n",
        "\n",
        "    for p in page_iter:\n",
        "        pagetxt = pages_text[p] if p < len(pages_text) else \"\"\n",
        "        # find explicit \"Schedule\" lines\n",
        "        for m in re.finditer(r'(Schedule\\s+.*)', pagetxt, re.IGNORECASE):\n",
        "            title_line = m.group(1).strip()\n",
        "            # pick the whole line\n",
        "            ln = title_line.splitlines()[0]\n",
        "            schedule_page_map[ln] = schedule_page_map.get(ln, []) + [p+1]\n",
        "    # Now parse pages to extract items: rely primarily on pdfplumber page tables if present\n",
        "    for p in tqdm(page_iter, desc=\"Parsing pages\", leave=False):\n",
        "        page_no = p+1\n",
        "        pt = pages_text[p] if p < len(pages_text) else \"\"\n",
        "        # Determine current schedule on this page via heuristics\n",
        "        # Look for lines like \"Item- <n> Schedule A1 (2.0 Earthwork)\" or \"Item- 1 Schedule A1\"\n",
        "        schedule_found = None\n",
        "        for line in pt.splitlines():\n",
        "            if re.search(r'Item[-\\s]*\\d+\\s*Schedule', line, re.IGNORECASE):\n",
        "                schedule_found = line.strip()\n",
        "                break\n",
        "            if re.search(r'Schedule\\s+A\\d*\\s*\\(.*\\)', line, re.IGNORECASE):\n",
        "                schedule_found = line.strip()\n",
        "                break\n",
        "            if re.search(r'Schedule\\s+A[-\\d\\w]*', line, re.IGNORECASE) and 'Item' in line:\n",
        "                schedule_found = line.strip()\n",
        "                break\n",
        "            # also look for \"Schedule A-All DSR 2021 Items\" heading\n",
        "            if re.search(r'Schedule\\s+.*DSR\\s*2021', line, re.IGNORECASE):\n",
        "                schedule_found = line.strip()\n",
        "                break\n",
        "        if schedule_found:\n",
        "            current_schedule = schedule_found\n",
        "        # If pdfplumber has table(s) on this page, attempt to parse meaningful ones\n",
        "        if (p+1) in pdfpl_tables and pdfpl_tables[p+1]:\n",
        "            for t in pdfpl_tables[p+1]:\n",
        "                # t is a table represented as list of lists\n",
        "                # convert to dataframe and try to detect columns\n",
        "                try:\n",
        "                    df = pd.DataFrame(t)\n",
        "                    # heuristics: if df has numeric columns near right side, try to set them as Rate/Amount\n",
        "                    # Normalize columns: merge multi-row headers\n",
        "                    # flatten header if it's repeating\n",
        "                    df = df.replace('', pd.NA).dropna(how='all', axis=1)\n",
        "                    # convert all to string temporarily, handling potential non-string data\n",
        "                    df2 = df.astype(str).applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "                    # Determine if this table contains item rows by scanning for numeric values in last columns\n",
        "                    right_numeric = False\n",
        "                    for col in df2.columns[-4:]:\n",
        "                        # check many numeric tokens\n",
        "                        numeric_count = df2[col].apply(lambda cell: bool(re.search(r'\\d', str(cell)))).sum()\n",
        "                        if numeric_count > 0:\n",
        "                            right_numeric = True\n",
        "                    if right_numeric:\n",
        "                        # attempt to set column names\n",
        "                        # try to locate header row by searching for 'S No' or 'Item No' or 'Description'\n",
        "                        headerRowIdx = None\n",
        "                        for ridx in range(min(3, len(df2))):\n",
        "                            rowtxt = ' '.join(df2.iloc[ridx].fillna('').tolist()).lower()\n",
        "                            if 'item no' in rowtxt or 'description' in rowtxt or 's no' in rowtxt or 'qty' in rowtxt:\n",
        "                                headerRowIdx = ridx\n",
        "                                break\n",
        "                        if headerRowIdx is not None:\n",
        "                            header = [str(x).strip() for x in df2.iloc[headerRowIdx].tolist()]\n",
        "                            data_df = df2.iloc[headerRowIdx+1:].copy()\n",
        "                            data_df.columns = header\n",
        "                            # Keep only likely useful columns and rename to standard columns\n",
        "                            std_cols = {}\n",
        "                            for c in data_df.columns:\n",
        "                                cc = str(c).lower() # Ensure column name is string\n",
        "                                if 's no' in cc or re.match(r'^\\s*s\\.?\\s*no', cc):\n",
        "                                    std_cols[c] = 'S No.'\n",
        "                                elif 'item' in cc and 'no' in cc:\n",
        "                                    std_cols[c] = 'Item No'\n",
        "                                elif 'description' in cc or 'desc' in cc:\n",
        "                                    std_cols[c] = 'Description of Item'\n",
        "                                elif 'unit' == cc.strip() or 'unit' in cc:\n",
        "                                    std_cols[c] = 'Unit'\n",
        "                                elif 'qty' in cc or 'quantity' in cc:\n",
        "                                    std_cols[c] = 'Qty'\n",
        "                                elif 'rate' in cc:\n",
        "                                    std_cols[c] = 'Rate'\n",
        "                                elif 'amount' in cc or 'value' in cc:\n",
        "                                    std_cols[c] = 'Amount'\n",
        "                                else:\n",
        "                                    # try to guess by cell examples\n",
        "                                    # if column contains mostly digits -> Rate/Amount/Qty\n",
        "                                    col_sample = data_df[c].astype(str).str.replace(',','').str.replace('₹','')\n",
        "                                    digits_frac = col_sample.str.match(r'^\\s*[\\d\\.\\-]+\\s*$').sum()\n",
        "                                    if digits_frac > len(data_df)/2:\n",
        "                                        # if already have Qty then this is Rate/Amount\n",
        "                                        if 'Qty' not in std_cols.values():\n",
        "                                            std_cols[c] = 'Qty'\n",
        "                                        elif 'Rate' not in std_cols.values():\n",
        "                                            std_cols[c] = 'Rate'\n",
        "                                        else:\n",
        "                                            std_cols[c] = 'Amount'\n",
        "                                    else:\n",
        "                                        std_cols[c] = c\n",
        "                            standardized = data_df.rename(columns=std_cols)\n",
        "                            # Ensure standardized has the required columns present (fallback to heuristics)\n",
        "                            for req in ['S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount']:\n",
        "                                if req not in standardized.columns:\n",
        "                                    standardized[req] = pd.NA\n",
        "                            # Trim rows where Item No and Description both are empty\n",
        "                            standardized = standardized[[ 'S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount']]\n",
        "                            # add schedule\n",
        "                            sched_key = current_schedule or f\"Page_{page_no}_table\"\n",
        "                            item_tables_dfs.setdefault(sched_key, []).append(standardized)\n",
        "                            progress_logs.append(f\"Parsed table on page {page_no} with pdfplumber -> schedule {sched_key}\")\n",
        "                except Exception as e:\n",
        "                    # ignore and continue\n",
        "                    progress_logs.append(f\"Table parse error on page {page_no}: {e}\")\n",
        "        # Fallback textual row parsing: find lines that look like item rows\n",
        "        # We'll parse line by line trying to detect rows with S No leading numeric\n",
        "        lines = [ln for ln in pt.splitlines() if ln.strip()]\n",
        "        for i, ln in enumerate(lines):\n",
        "            # detect start of an item block like \"Item- 1 Schedule A1 (2.0 Earthwork)\" or \"Item- 1 Schedule A1\"\n",
        "            if re.match(r'Item[-\\s]*\\d+\\s*Schedule', ln, re.IGNORECASE):\n",
        "                # set current schedule based on this line (prefer parentheses)\n",
        "                current_schedule = ln.strip()\n",
        "                continue\n",
        "            # detect item rows: start with number optionally then item number e.g., \"1 2.6.1 Earth work ... cum 1600 205.45 328720\"\n",
        "            mrow = re.match(r'^\\s*(\\d+)\\s+([0-9\\.]+)\\s+(.*)$', ln)\n",
        "            if mrow:\n",
        "                sno = mrow.group(1)\n",
        "                item_no = mrow.group(2)\n",
        "                rest = mrow.group(3)\n",
        "                # attempt to extract unit, qty, rate, amount from end of rest\n",
        "                nums, tokens = numeric_tokens_from_line(rest)\n",
        "                # numeric_tokens may return last numeric tokens\n",
        "                unit = None; qty=None; rate=None; amount=None; desc=None\n",
        "                if nums and len(nums) >= 3:\n",
        "                    # choose last numeric as amount, previous as rate, earlier as qty\n",
        "                    try:\n",
        "                        # find positions of these tokens in original split tokens\n",
        "                        amount = nums[-1][1]\n",
        "                        rate = nums[-2][1] if len(nums)>=2 else None\n",
        "                        qty = nums[-3][1] if len(nums)>=3 else None\n",
        "                        # Unit is token just before qty in tokens if present\n",
        "                        # reconstruct description by removing numeric tokens at end\n",
        "                        # convert tokens to list\n",
        "                        toks = re.split(r'\\s{2,}|\\t', rest)\n",
        "                        if len(toks) < 4:\n",
        "                            toks = re.split(r'\\s+', rest)\n",
        "                        # Remove trailing numeric tokens from rest to get description\n",
        "                        desc = rest\n",
        "                        # strip numeric matches from end\n",
        "                        desc = re.sub(r'(\\s*[\\d\\.,]+\\s*)+$', '', desc).strip()\n",
        "                        # attempt to find unit as the token just before Qty number string\n",
        "                        # find original qty token string\n",
        "                        qty_token_str = nums[-3][0] if len(nums)>=3 else None\n",
        "                        if qty_token_str:\n",
        "                            # locate in rest\n",
        "                            idx = rest.rfind(qty_token_str)\n",
        "                            if idx != -1:\n",
        "                                left = rest[:idx].strip()\n",
        "                                # last token of left likely is Unit\n",
        "                                u_toks = left.split()\n",
        "                                if u_toks:\n",
        "                                    unit = u_toks[-1]\n",
        "                                    # remove unit from desc if appended\n",
        "                                    if desc.endswith(unit):\n",
        "                                        desc = desc[: -len(unit)].strip()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                else:\n",
        "                    # try other pattern: some rows have 'cum 1600 205.45 328720' with item no on previous line\n",
        "                    parts = rest.split()\n",
        "                    # attempt to find first numeric token from right side\n",
        "                    right_nums = [to_number(tok) for tok in reversed(parts)]\n",
        "                    right_nums = [r for r in right_nums if r is not None]\n",
        "                    if len(right_nums) >= 1:\n",
        "                        amount = right_nums[0]\n",
        "                    if len(right_nums) >= 2:\n",
        "                        rate = right_nums[1]\n",
        "                    if len(right_nums) >= 3:\n",
        "                        qty = right_nums[2]\n",
        "                    # attempt to find unit as first non-numeric near the right\n",
        "                    unit = None\n",
        "                    for tok in reversed(parts):\n",
        "                        if to_number(tok) is None:\n",
        "                            unit = tok\n",
        "                            break\n",
        "                    desc = rest\n",
        "                row = {\n",
        "                    'S No.': sno,\n",
        "                    'Item No': item_no,\n",
        "                    'Description of Item': desc,\n",
        "                    'Unit': unit,\n",
        "                    'Qty': qty,\n",
        "                    'Rate': rate,\n",
        "                    'Amount': amount,\n",
        "                    'Page': page_no\n",
        "                }\n",
        "                key = current_schedule or f\"Schedule_Page_{page_no}\"\n",
        "                item_breakups[key].append(row)\n",
        "            else:\n",
        "                # Other common pattern: lines starting with item number like \"2 2.33.1 Beyond 30 cm...\"\n",
        "                m2 = re.match(r'^\\s*([0-9]+)\\s+([0-9]+\\.[0-9]+\\.[0-9]+)\\s+(.*)$', ln)\n",
        "                if m2:\n",
        "                    sno = m2.group(1); item_no = m2.group(2); rest = m2.group(3)\n",
        "                    nums, tokens = numeric_tokens_from_line(rest)\n",
        "                    qty=None; rate=None; amount=None; unit=None; desc=rest\n",
        "                    if nums and len(nums)>=3:\n",
        "                        amount = nums[-1][1]; rate = nums[-2][1]; qty = nums[-3][1]\n",
        "                        desc = re.sub(r'(\\s*[\\d\\.,]+\\s*)+$', '', rest).strip()\n",
        "                        # try to deduce unit by finding token before qty token\n",
        "                        # simple heuristics: last word before qty number is unit in many lines\n",
        "                        # use tokens split on spaces\n",
        "                        tks = re.split(r'\\s+', rest)\n",
        "                        # find position of first occurrence of str(qty) from right\n",
        "                        qstr = str(int(qty)) if isinstance(qty, (int,float)) and float(qty).is_integer() else str(qty)\n",
        "                        pos = None\n",
        "                        for ix in range(len(tks)-1,-1,-1):\n",
        "                            if re.sub(r'[^\\d\\.\\-]','', tks[ix]) == qstr:\n",
        "                                pos = ix\n",
        "                                break\n",
        "                        if pos and pos-1 >= 0:\n",
        "                            unit = tks[pos-1]\n",
        "                    row = {\n",
        "                        'S No.': sno,\n",
        "                        'Item No': item_no,\n",
        "                        'Description of Item': desc,\n",
        "                        'Unit': unit,\n",
        "                        'Qty': qty,\n",
        "                        'Rate': rate,\n",
        "                        'Amount': amount,\n",
        "                        'Page': page_no\n",
        "                    }\n",
        "                    key = current_schedule or f\"Schedule_Page_{page_no}\"\n",
        "                    item_breakups[key].append(row)\n",
        "    # convert collected lists into DataFrames and normalize types\n",
        "    normalized_item_tables = {}\n",
        "    for k, rows in item_breakups.items():\n",
        "        if isinstance(rows, list) and rows:\n",
        "            df = pd.DataFrame(rows)\n",
        "            # ensure columns exist\n",
        "            for col in ['S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount']:\n",
        "                if col not in df.columns:\n",
        "                    df[col] = pd.NA\n",
        "            # coerce numeric columns\n",
        "            df['Qty'] = df['Qty'].apply(lambda x: to_number(x))\n",
        "            df['Rate'] = df['Rate'].apply(lambda x: to_number(x))\n",
        "            df['Amount'] = df['Amount'].apply(lambda x: to_number(x))\n",
        "            normalized_item_tables[k] = df[['S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount', 'Page']]\n",
        "    # Also integrate tables parsed via pdfplumber converted to dataframes earlier (item_tables_dfs)\n",
        "    for sched_k, list_of_dfs in item_tables_dfs.items():\n",
        "        combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "        # Ensure required columns exist before processing\n",
        "        for col in ['S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount']:\n",
        "            if col not in combined_df.columns:\n",
        "                combined_df[col] = pd.NA\n",
        "        # Convert numeric columns to appropriate types, coercing errors\n",
        "        for col in ['Qty', 'Rate', 'Amount']:\n",
        "             combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
        "        # Keep only the desired columns and assign to normalized_item_tables\n",
        "        normalized_item_tables[sched_k] = combined_df[['S No.', 'Item No', 'Description of Item', 'Unit', 'Qty', 'Rate', 'Amount']]\n",
        "\n",
        "    parse_results['item_breakups'] = normalized_item_tables\n",
        "    confidence_scores['item_breakups'] = 90 if normalized_item_tables else 40\n",
        "    progress_logs.append(f\"Extracted item breakups for {len(normalized_item_tables)} schedules.\")\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"Item breakup extraction error: {e}\\n{traceback.format_exc()}\")\n",
        "    parse_results['item_breakups'] = {}\n",
        "    confidence_scores['item_breakups'] = 30\n",
        "\n",
        "# ------------------------------\n",
        "# GROUP Schedule A perfectly by parent class using regex\n",
        "# ------------------------------\n",
        "grouped_schedules = {}\n",
        "try:\n",
        "    # For each schedule key, attempt to detect parent class via pattern\n",
        "    pattern = re.compile(r\"(\\d+\\.\\d+)\\s+([A-Z0-9\\.\\s&\\-]+(?:WORK|CONCRETE|STEEL|WATER|DRAINAGE|ROOFING|MASONRY|PLASTER|CLADDING|FINISHING)?)\", re.IGNORECASE)\n",
        "    for sched_key, df in parse_results['item_breakups'].items():\n",
        "        # try to search in schedule key name first\n",
        "        m = pattern.search(sched_key)\n",
        "        groupname = None\n",
        "        if m:\n",
        "            groupname = m.group(2).strip().upper()\n",
        "        else:\n",
        "            # search inside descriptions for strong occurrence of parent header like \"5.0 REINFORCED CEMENT CONCRETE\"\n",
        "            found = None\n",
        "            for text in parse_results['raw_text_pages'][:8]:\n",
        "                mm = pattern.search(text)\n",
        "                if mm:\n",
        "                    found = mm.group(2).strip().upper()\n",
        "                    break\n",
        "            groupname = found or sched_key\n",
        "        # Normalize group name\n",
        "        group_label = f\"{sched_key} | {groupname}\"\n",
        "        grouped_schedules[group_label] = df\n",
        "    parse_results['item_breakups_grouped'] = grouped_schedules\n",
        "    progress_logs.append(\"Grouped Schedule A entries by parent class using regex.\")\n",
        "    confidence_scores['grouping'] = 92\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"Schedule grouping error: {e}\")\n",
        "    parse_results['item_breakups_grouped'] = parse_results['item_breakups']\n",
        "    confidence_scores['grouping'] = 40\n",
        "\n",
        "# ------------------------------\n",
        "# ELIGIBILITY CRITERIA (search pages 15-35)\n",
        "# ------------------------------\n",
        "eligibility_bullets = []\n",
        "elig_raw = []\n",
        "try:\n",
        "    # pages are 1-indexed to user; convert to 0-index\n",
        "    start = max(0, 14)\n",
        "    end = min(len(pages_text), 35)\n",
        "    block = '\\n'.join(pages_text[start:end])\n",
        "    parse_results['eligibility_raw_block'] = block\n",
        "    # search for headings\n",
        "    keywords = ['ELIGIBILITY', 'QUALIFICATION', 'FINANCIAL CRITERIA']\n",
        "    # split into lines and pick lines that look like bullet points or contain keywords\n",
        "    lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n",
        "    for i, ln in enumerate(lines):\n",
        "        uln = ln.upper()\n",
        "        if any(k in uln for k in keywords) or re.match(r'^\\d+\\.', ln) or re.match(r'^[A-Za-z]\\)', ln) or ln.startswith('-') or ln.startswith('*'):\n",
        "            # take this and next few lines as a bullet chunk\n",
        "            chunk = ln\n",
        "            # append subsequent short lines until next numbered heading\n",
        "            j = i+1\n",
        "            while j < len(lines) and len(lines[j]) < 200 and not re.match(r'^\\d+\\.', lines[j]):\n",
        "                # stop if blank or uppercase title\n",
        "                if lines[j].isupper() and len(lines[j].split())<8:\n",
        "                    break\n",
        "                chunk += ' ' + lines[j]\n",
        "                j += 1\n",
        "            eligibility_bullets.append(chunk.strip())\n",
        "    # normalize bullets: remove long legal paragraphs & keep shorter bullets\n",
        "    filtered = []\n",
        "    for b in eligibility_bullets:\n",
        "        if len(b.split()) > 6 and len(b.split()) < 200:\n",
        "            filtered.append(b)\n",
        "    # fallback: if none found, extract all text between \"4. ELIGIBILITY CONDITIONS\" and \"5. COMPLIANCE\"\n",
        "    if not filtered:\n",
        "        m = re.search(r'4\\.\\s*ELIGIBILITY CONDITIONS(.*)5\\.\\s*COMPLIANCE', block, re.IGNORECASE | re.DOTALL)\n",
        "        if m:\n",
        "            blk = m.group(1)\n",
        "            bullets = [ln.strip() for ln in blk.splitlines() if ln.strip()]\n",
        "            filtered = bullets\n",
        "    parse_results['eligibility_criteria'] = {'bullets': filtered, 'raw_text': block}\n",
        "    confidence_scores['eligibility'] = 90 if filtered else 45\n",
        "    progress_logs.append(f\"Extracted {len(filtered)} eligibility bullets from pages 15-35.\")\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"Eligibility extraction error: {e}\")\n",
        "    parse_results['eligibility_criteria'] = {'bullets': [], 'raw_text': ''}\n",
        "    confidence_scores['eligibility'] = 30\n",
        "\n",
        "# ------------------------------\n",
        "# TOP 10 COST DRIVERS (global)\n",
        "# ------------------------------\n",
        "top10_list = []\n",
        "try:\n",
        "    # Combine all item_breakups and compute amounts per top-level description tokens\n",
        "    all_items = []\n",
        "    for sched_label, df in parse_results['item_breakups_grouped'].items():\n",
        "        if isinstance(df, pd.DataFrame):\n",
        "            tmp = df.copy()\n",
        "            tmp['Schedule'] = sched_label\n",
        "            all_items.append(tmp)\n",
        "    if all_items:\n",
        "        big = pd.concat(all_items, ignore_index=True, sort=False)\n",
        "        # ensure Amount numeric and handle potential non-numeric values\n",
        "        big['Amount'] = pd.to_numeric(big['Amount'], errors='coerce').fillna(0.0) # Ensure fillna after to_numeric\n",
        "        # Create a coarse \"Category\" using Item No or Schedule header or Description prefix\n",
        "        def category_from_row(r):\n",
        "            # prefer schedule parent if includes keywords like R.C.C\n",
        "            sched = r.get('Schedule') or ''\n",
        "            # attempt to extract e.g., \"REINFORCED CEMENT CONCRETE\"\n",
        "            m = parent_work_re.search(sched)\n",
        "            if m:\n",
        "                return m.group(2).strip().upper()\n",
        "            # fallback: look into Description for keywords\n",
        "            desc = str(r.get('Description of Item') or '').upper()\n",
        "            if 'CONCRETE' in desc:\n",
        "                return 'R.C.C WORK / CONCRETE'\n",
        "            if 'STEEL' in desc or 'REINFORCEMENT' in desc:\n",
        "                return 'STEEL / REINFORCEMENT'\n",
        "            if 'EARTH' in desc or 'EXCAVATION' in desc:\n",
        "                return 'EARTHWORK'\n",
        "            # fallback to first words of schedule label\n",
        "            return sched.split('|')[-1].strip()[:40].upper()\n",
        "        big['Category'] = big.apply(category_from_row, axis=1)\n",
        "        # Drop rows where 'Category' is None before grouping\n",
        "        big = big.dropna(subset=['Category'])\n",
        "        catsum = big.groupby('Category', dropna=False)['Amount'].sum().reset_index().sort_values('Amount', ascending=False)\n",
        "        total_amt = catsum['Amount'].sum() if not catsum.empty else 0.0\n",
        "        topk = catsum.head(10)\n",
        "        # For top items, also extract prominent subitems (by description) within that category\n",
        "        top10_result = []\n",
        "        for _, row in topk.iterrows():\n",
        "            cat = row['Category']\n",
        "            amt = row['Amount']\n",
        "            pct = (amt/total_amt*100) if total_amt>0 else 0.0\n",
        "            sub = big[big['Category']==cat].copy()\n",
        "            # find top 2 descriptions by amount\n",
        "            # Ensure 'Description of Item' exists and handle potential non-string data\n",
        "            if 'Description of Item' in sub.columns:\n",
        "                 sub['Description of Item'] = sub['Description of Item'].astype(str)\n",
        "            else:\n",
        "                 sub['Description of Item'] = '' # Add column if missing\n",
        "            subsum = sub.groupby('Description of Item')['Amount'].sum().reset_index().sort_values('Amount', ascending=False).head(5)\n",
        "            top10_result.append({'Category': cat, 'Amount': amt, 'Pct': pct, 'TopDescriptions': list(subsum.to_dict('records'))})\n",
        "        parse_results['top10'] = {'total': total_amt, 'top10': top10_result}\n",
        "        confidence_scores['top10'] = 88 if top10_result else 40\n",
        "        progress_logs.append(\"Computed top 10 cost drivers.\")\n",
        "    else:\n",
        "        parse_results['top10'] = {'total':0,'top10':[]}\n",
        "        confidence_scores['top10'] = 20\n",
        "except Exception as e:\n",
        "    progress_logs.append(f\"Top10 computation error: {e}\")\n",
        "    parse_results['top10'] = {'total':0,'top10':[]}\n",
        "    confidence_scores['top10'] = 20\n",
        "\n",
        "# ------------------------------\n",
        "# RISK ANALYSIS heuristics (simple)\n",
        "# ------------------------------\n",
        "risk_notes = []\n",
        "try:\n",
        "    # Heuristics: if any item description mentions 'dismantling' with qty large -> high safety risk\n",
        "    for sched, df in parse_results['item_breakups_grouped'].items():\n",
        "        if isinstance(df, pd.DataFrame):\n",
        "            for desc, qty in zip(df['Description of Item'].fillna(''), df['Qty'].fillna(0)):\n",
        "                d = str(desc).lower()\n",
        "                if 'dismantl' in d or 'demolis' in d:\n",
        "                    if qty and to_number(qty) and to_number(qty) > 10:\n",
        "                        risk_notes.append(f\"Dismantling {qty} units → High safety risk\")\n",
        "                if 'steel' in d or 'thermo' in d or 'fe-500' in d:\n",
        "                    # aggregate steel qty\n",
        "                    risk_notes.append(\"Steel items present → Price volatility alert\")\n",
        "                if 'months' in d or 'period of completion' in d:\n",
        "                    # capture timeline\n",
        "                    pass\n",
        "    # timeline check from nit header\n",
        "    period = parse_results['nit_header'].get('Period of Completion','')\n",
        "    if period:\n",
        "        risk_notes.append(f\"Timeline: {period} → check feasibility\")\n",
        "    # add some static heuristics\n",
        "    risk_notes = list(dict.fromkeys(risk_notes))  # unique\n",
        "    parse_results['risk_analysis'] = risk_notes\n",
        "    confidence_scores['risk'] = 70\n",
        "except Exception:\n",
        "    parse_results['risk_analysis'] = []\n",
        "    confidence_scores['risk'] = 30\n",
        "\n",
        "# ------------------------------\n",
        "# FLAGS (JV NOT ALLOWED, Single Packet System, Earnest Money)\n",
        "# ------------------------------\n",
        "flags = []\n",
        "try:\n",
        "    header = parse_results['nit_header']\n",
        "    # JV NOT ALLOWED\n",
        "    jv_allowed = header.get('Are JV allowed to bid') or ''\n",
        "    if jv_allowed.strip().lower() in ['no','not allowed','n']:\n",
        "        flags.append({'text':'JV NOT ALLOWED', 'severity':'red'})\n",
        "    # Single Packet System: look in raw text for token\n",
        "    whole_text = '\\n'.join(pages_text[:6])\n",
        "    if re.search(r'Single Packet System', whole_text, re.IGNORECASE):\n",
        "        flags.append({'text':'Single Packet System', 'severity':'red'})\n",
        "    # Earnest Money\n",
        "    em = header.get('Earnest Money') or ''\n",
        "    if em:\n",
        "        # format in INR with commas\n",
        "        em_num = to_number(em)\n",
        "        try:\n",
        "            em_fmt = f\"₹{int(em_num):,}\" if em_num else em\n",
        "        except Exception:\n",
        "            em_fmt = em\n",
        "        flags.append({'text':f\"Earnest Money: {em_fmt}\", 'severity':'red'})\n",
        "    parse_results['flags'] = flags\n",
        "    confidence_scores['flags'] = 95 if flags else 40\n",
        "except Exception as e:\n",
        "    parse_results['flags'] = []\n",
        "    confidence_scores['flags'] = 40\n",
        "    progress_logs.append(f\"Flags detection error: {e}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Prepare Beautiful Markdown View\n",
        "# ------------------------------\n",
        "try:\n",
        "    # Header top line\n",
        "    tender_no = parse_results['nit_header'].get('Tender No') or os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "    # Advertised value guess\n",
        "    adv_val = parse_results['nit_header'].get('Advertised Value') or ''\n",
        "    # try to format rupee\n",
        "    adv_amt = to_number(adv_val) or None\n",
        "    adv_str = f\"₹{adv_amt:,.2f}\" if adv_amt is not None else adv_val\n",
        "    # Title (try to extract station or short from Name of Work)\n",
        "    name_work = parse_results['nit_header'].get('Name of Work') or ''\n",
        "    title_line = f\"# TENDER {tender_no} | {name_work[:30]} | {adv_str}\"\n",
        "    md_lines = [title_line, '\\n']\n",
        "    # NIT HEADER table\n",
        "    md_lines.append(\"## NIT HEADER\")\n",
        "    # convert nit header dict into two-column table\n",
        "    nit = parse_results['nit_header']\n",
        "    if nit:\n",
        "        md_lines.append(\"| Field | Value |\")\n",
        "        md_lines.append(\"|---|---|\")\n",
        "        for k,v in nit.items():\n",
        "            md_lines.append(f\"| {k} | {str(v).replace('|','\\\\|')} |\")\n",
        "    else:\n",
        "        md_lines.append(\"_No header parsed._\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # Schedule summary\n",
        "    md_lines.append(\"## SCHEDULE SUMMARY\")\n",
        "    ss = parse_results['schedules_summary']\n",
        "    if isinstance(ss, pd.DataFrame):\n",
        "        try:\n",
        "            md_lines.append(ss.to_markdown(index=False))\n",
        "        except Exception:\n",
        "            md_lines.append(\"`Schedule summary present (DataFrame)`\")\n",
        "    else:\n",
        "        md_lines.append(\"_Schedule summary not parsed into table. Raw snippet:_\")\n",
        "        md_lines.append(\"```\\n\" + (str(ss)[:400] if ss else \"None\") + \"\\n```\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # TOP 10 COST DRIVERS\n",
        "    md_lines.append(\"## TOP 10 COST DRIVERS (Global)\")\n",
        "    top10 = parse_results.get('top10', {})\n",
        "    total_amt = top10.get('total', 0)\n",
        "    if top10 and top10.get('top10'):\n",
        "        n=1\n",
        "        for item in top10['top10']:\n",
        "            cat = item['Category']\n",
        "            amt = item['Amount'] or 0\n",
        "            pct = item['Pct'] or 0.0\n",
        "            md_lines.append(f\"{n}. {cat} → {pct:.2f}% (₹{amt:,.2f})\")\n",
        "            # list top descriptions (limit 3)\n",
        "            for td in item.get('TopDescriptions', [])[:3]:\n",
        "                desc = td.get('Description of Item') or td.get('Description','') # Use get with default\n",
        "                a = td.get('Amount') or 0\n",
        "                # attempt to get qty and unit by searching big DF\n",
        "                md_lines.append(f\"   ├─ {desc[:60]} → ₹{a:,.2f}\")\n",
        "            n += 1\n",
        "    else:\n",
        "        md_lines.append(\"_No cost drivers computed._\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # ELIGIBILITY\n",
        "    md_lines.append(\"## ELIGIBILITY CRITERIA\")\n",
        "    bullets = parse_results.get('eligibility_criteria', {}).get('bullets', [])\n",
        "    raw_elig = parse_results.get('eligibility_criteria', {}).get('raw_text', '')\n",
        "    if bullets:\n",
        "        for b in bullets:\n",
        "            md_lines.append(f\"- {b}\")\n",
        "    else:\n",
        "        md_lines.append(\"_No bulletized eligibility criteria found; raw text below:_\")\n",
        "        md_lines.append(\"```\")\n",
        "        md_lines.append((raw_elig[:800] + '...') if len(raw_elig)>800 else raw_elig)\n",
        "        md_lines.append(\"```\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # RISK ANALYSIS\n",
        "    md_lines.append(\"## RISK ANALYSIS\")\n",
        "    risks = parse_results.get('risk_analysis', [])\n",
        "    if risks:\n",
        "        for r in risks:\n",
        "            md_lines.append(f\"• {r}\")\n",
        "    else:\n",
        "        md_lines.append(\"• _No automatic risks detected. Manual review recommended._\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # FLAGS\n",
        "    md_lines.append(\"## FLAGS\")\n",
        "    for f in parse_results.get('flags', []):\n",
        "        # mark in red using HTML (Markdown doesn't have red text natively)\n",
        "        md_lines.append(f\"<span style='color:red; font-weight:bold'>{f['text']}</span>\")\n",
        "    md_lines.append(\"\\n\")\n",
        "    # CONFIDENCE + LOGS\n",
        "    md_lines.append(\"## PARSING LOGS & CONFIDENCE\")\n",
        "    md_lines.append(\"**Confidence scores:**\")\n",
        "    for k,v in confidence_scores.items():\n",
        "        md_lines.append(f\"- {k}: {v}%\")\n",
        "    md_lines.append(\"\\n**Progress logs (highlights):**\")\n",
        "    for pl in progress_logs[-12:]:\n",
        "        md_lines.append(f\"- {pl}\")\n",
        "    # Render Markdown\n",
        "    md_full = '\\n'.join(md_lines)\n",
        "    display(Markdown(md_full))\n",
        "except Exception as e:\n",
        "    print(\"Error while preparing markdown view:\", e)\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "# ------------------------------\n",
        "# EXPORTS: top10.csv, full_breakup.json, parsed_data.pkl\n",
        "# ------------------------------\n",
        "# Create top10.csv (simple flattened)\n",
        "try:\n",
        "    top10_flat = []\n",
        "    for entry in parse_results.get('top10', {}).get('top10', []):\n",
        "        cat = entry['Category']\n",
        "        amt = entry['Amount']\n",
        "        pct = entry['Pct']\n",
        "        # include top descriptions\n",
        "        for td in entry.get('TopDescriptions', []):\n",
        "            top10_flat.append({\n",
        "                'Category': cat,\n",
        "                'CategoryAmount': amt,\n",
        "                'CategoryPct': pct,\n",
        "                'Description': td.get('Description of Item') or td.get('Description',''), # Use get with default\n",
        "                'DescAmount': td.get('Amount') or 0\n",
        "            })\n",
        "    top10_df = pd.DataFrame(top10_flat)\n",
        "    top10_csv = 'top10.csv'\n",
        "    top10_df.to_csv(top10_csv, index=False)\n",
        "    print(f\"Saved {top10_csv}\")\n",
        "except Exception as e:\n",
        "    print(\"top10.csv creation failed:\", e)\n",
        "\n",
        "# full_breakup.json\n",
        "try:\n",
        "    # prepare serializable dict for item_breakups_grouped\n",
        "    ser = {}\n",
        "    for k, df in parse_results.get('item_breakups_grouped', {}).items():\n",
        "        try:\n",
        "            # Convert DataFrame to dictionary, handling potential non-serializable types\n",
        "            ser[k] = df.fillna('').to_dict(orient='records')\n",
        "        except Exception:\n",
        "            ser[k] = []\n",
        "    full_json_path = 'full_breakup.json'\n",
        "    with open(full_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            'nit_header': parse_results.get('nit_header', {}),\n",
        "            'schedules_summary': None if parse_results.get('schedules_summary') is None else (parse_results['schedules_summary'].to_dict('records') if isinstance(parse_results['schedules_summary'], pd.DataFrame) else str(parse_results['schedules_summary'])),\n",
        "            'item_breakups_grouped': ser,\n",
        "            'eligibility_criteria': parse_results.get('eligibility_criteria', {}),\n",
        "            'top10': parse_results.get('top10', {}),\n",
        "            'flags': parse_results.get('flags', [])\n",
        "        }, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved {full_json_path}\")\n",
        "except Exception as e:\n",
        "    print(\"full_breakup.json creation failed:\", e)\n",
        "\n",
        "# parsed_data.pkl\n",
        "try:\n",
        "    pkl_path = 'parsed_data.pkl'\n",
        "    with open(pkl_path, 'wb') as f:\n",
        "        pickle.dump(parse_results, f)\n",
        "    print(f\"Saved {pkl_path}\")\n",
        "except Exception as e:\n",
        "    print(\"parsed_data.pkl creation failed:\", e)\n",
        "\n",
        "# ------------------------------\n",
        "# Provide export buttons (downloads)\n",
        "# ------------------------------\n",
        "print(\"\\nExport files available for download:\")\n",
        "for path in ['top10.csv', 'full_breakup.json', 'parsed_data.pkl']:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"- {path}\")\n",
        "    else:\n",
        "        print(f\"- {path} (missing)\")\n",
        "\n",
        "# Attempt to provide automatic downloads (Colab will prompt)\n",
        "try:\n",
        "    # Use download but wrap in try/except to avoid crashes\n",
        "    print(\"\\nAttempting to trigger downloads (browser will prompt)...\")\n",
        "    for path in ['top10.csv', 'full_breakup.json', 'parsed_data.pkl']:\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                files.download(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not auto-download {path}: {e}\")\n",
        "except Exception as e:\n",
        "    print(\"Auto-download failed:\", e)\n",
        "\n",
        "# ------------------------------\n",
        "# PROGRESS SUMMARY (display)\n",
        "# ------------------------------\n",
        "print(\"\\n--- PARSING SUMMARY ---\")\n",
        "print(f\"File: {pdf_path}\")\n",
        "print(f\"Pages processed: {len(pages_text)}\")\n",
        "print(\"Confidence scores:\")\n",
        "for k,v in confidence_scores.items():\n",
        "    print(f\" - {k}: {v}%\")\n",
        "print(\"\\nTop logs:\")\n",
        "for ln in progress_logs[-10:]:\n",
        "    print(\" -\", ln)\n",
        "\n",
        "# Ensure notebook never crashes: final safety output of partial results counts\n",
        "print(\"\\nPartial counts (guaranteed):\")\n",
        "print(\" - NIT header fields:\", len(parse_results.get('nit_header', {})))\n",
        "print(\" - Schedule summaries parsed:\", 0 if parse_results.get('schedules_summary') is None else (len(parse_results['schedules_summary']) if isinstance(parse_results['schedules_summary'], pd.DataFrame) else 1))\n",
        "print(\" - Item breakup schedules:\", len(parse_results.get('item_breakups_grouped', {})))\n",
        "print(\" - Eligibility bullets:\", len(parse_results.get('eligibility_criteria', {}).get('bullets', [])))\n",
        "\n",
        "# Save a small human-readable markdown export too\n",
        "try:\n",
        "    with open('parsed_summary.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(md_full)\n",
        "    print(\"Saved parsed_summary.md\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# STEP COMPLETE\n",
        "print(\"\\nAnalysis complete. Use the downloaded files for further inspection.\")\n",
        "# End of script\n",
        "\n",
        "# Suggestions for next actions (short; as comments)\n",
        "# **a.** (Optional) Run the notebook's parsing cell again with `tabula` available in the environment and Java enabled for likely improved table extraction.\n",
        "# **b.** (Optional) Ask to add unit-tests for parser functions or to export Excel (xlsx) with separate sheets per schedule."
      ]
    }
  ]
}